# sat_comm_server.py
from __future__ import annotations

import os
import json
import asyncio
import threading
import logging
import copy
import sys
import time, random
import importlib
import math, re

from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, Future
from contextlib import asynccontextmanager
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from collections import Counter

from fastapi import FastAPI, Query
from fastapi.responses import HTMLResponse
from skyfield.api import load, EarthSatellite, Topos
from pathlib import Path

# --- data registry (ë¡œì»¬ CIFAR ë¶„ë°° ëª¨ë“ˆ) ---
from data import (
    CIFAR_ROOT, ASSIGNMENTS_DIR, SAMPLES_PER_CLIENT, DIRICHLET_ALPHA, RNG_SEED, WITH_REPLACEMENT,
    DATA_REGISTRY, get_training_dataset,
)

# -------------------- Optional: torch for GPU detection --------------------
try:
    import torch, torch.nn as nn
    _has_torch = True
except Exception:
    _has_torch = False

if _has_torch:
    import torch.multiprocessing as mp
    try:
        # CUDAì™€ í•¨ê»˜ forkëŠ” ìœ„í—˜. spawnì„ ê°•ì œí•˜ê³  ê³µìœ ì „ëµì„ íŒŒì¼ì‹œìŠ¤í…œìœ¼ë¡œ.
        if mp.get_start_method(allow_none=True) != "spawn":
            mp.set_start_method("spawn", force=True)
        torch.multiprocessing.set_sharing_strategy("file_system")
    except RuntimeError:
        pass

# -------------------- Logger --------------------
logger = logging.getLogger("simserver")
logger.setLevel(logging.INFO)
if not logger.handlers:
    h = logging.StreamHandler()
    h.setFormatter(logging.Formatter("[%(levelname)s] %(message)s"))
    logger.addHandler(h)

def _log(msg: str):
    logger.info(f"[FL] {msg}")

# -------------------- í™˜ê²½/ê²½ë¡œ --------------------
def _is_wsl() -> bool:
    try:
        return "microsoft" in Path("/proc/version").read_text().lower()
    except Exception:
        return False
    
# DataLoader workers: WSL ë˜ëŠ” reload í™˜ê²½ì´ë©´ 0ìœ¼ë¡œ(ë©€í‹°í”„ë¡œì„¸ì‹± ë¹„í™œì„±í™”)
DEFAULT_DL_WORKERS = 0 if _is_wsl() or os.getenv("UVICORN_RELOAD") else 2
DL_WORKERS = int(os.getenv("FL_DATALOADER_WORKERS", str(DEFAULT_DL_WORKERS)))

# ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬: simserver/ckpt
BASE_DIR = Path(__file__).resolve().parent
CKPT_DIR = BASE_DIR / "ckpt"
CKPT_DIR.mkdir(parents=True, exist_ok=True)

LAST_GLOBAL_PTR = CKPT_DIR / "last_global.json"

# --- Metrics paths ---
METRICS_DIR = BASE_DIR / "metrics"
GLOBAL_METRICS_DIR = METRICS_DIR / "global"
GLOBAL_METRICS_DIR.mkdir(parents=True, exist_ok=True)
LOCAL_METRICS_DIR = METRICS_DIR / "local"
LOCAL_METRICS_DIR.mkdir(parents=True, exist_ok=True)
METRICS_DIR.mkdir(parents=True, exist_ok=True)

GLOBAL_METRICS_CSV = GLOBAL_METRICS_DIR / "global_metrics.csv"
GLOBAL_METRICS_XLSX = GLOBAL_METRICS_DIR / "global_metrics.xlsx"
GLOBAL_METRICS_HEADER = [
    "timestamp", "version", "split", "num_samples",
    "loss", "acc", "f1_macro", "madds_M", "flops_M", "latency_ms",
    "ckpt_path"
]

try:
    import pandas as _pd  # optional
    _has_pandas = True
except Exception:
    _has_pandas = False
    
METRICS_LOCK = threading.Lock()  # íŒŒì¼ append ë™ì‹œì„± ë³´í˜¸

# ---- ê¸€ë¡œë²Œ ëª¨ë¸ ì§‘ê³„ ìƒíƒœ ----
GLOBAL_MODEL_LOCK = threading.Lock()
GLOBAL_MODEL_STATE = None       # latest state_dict (CPU í…ì„œ)
GLOBAL_MODEL_VERSION = -1       # -1ì´ë©´ ì•„ì§ ì´ˆê¸°í™” ì „
AGG_ALPHA = float(os.getenv("FL_AGG_ALPHA", "0.2"))  # (1-Î±)G + Î±L ì˜ Î±
EVAL_EVERY_N = int(os.getenv("FL_EVAL_EVERY_N", "2"))  # ê¸€ë¡œë²Œ vê°€ Në°°ìˆ˜ì¼ ë•Œë§Œ í‰ê°€
EVAL_BS = int(os.getenv("FL_EVAL_BS", "1024"))

STALENESS_TAU = float(os.getenv("FL_STALENESS_TAU", "14"))     # ê°ì‡  ì†ë„
STALENESS_MODE = os.getenv("FL_STALENESS_MODE", "exp")         # "exp" or "poly"
W_MIN = float(os.getenv("FL_STALENESS_W_MIN", "0.02"))          # ë°”ë‹¥ ê°€ì¤‘ì¹˜(ì„ íƒ)
S_MAX_DROP = int(os.getenv("FL_STALENESS_MAX_DROP", "0"))      # 0ì´ë©´ ë“œë ì•ˆí•¨
ALPHA_MAX = float(os.getenv("FL_AGG_ALPHA_MAX", "0.5"))

import re, math
_fromg_re = re.compile(r"fromg(\d+)")

# ==================== ì‹œë®¬ë ˆì´ì…˜ ìƒíƒœ ë³€ìˆ˜ ====================
satellites: Dict[int, EarthSatellite] = {}         # sat_id -> EarthSatellite
sat_comm_status: Dict[int, bool] = {}              # sat_id -> í†µì‹  ê°€ëŠ¥ ì—¬ë¶€
current_sat_positions: Dict[int, Dict[str, float]] = {}

observer_locations = {
    "Berlin":  Topos(latitude_degrees=52.52,  longitude_degrees=13.41,  elevation_m=34),
    "Houston": Topos(latitude_degrees=29.76,  longitude_degrees=-95.37, elevation_m=30),
    "Tokyo":   Topos(latitude_degrees=35.68,  longitude_degrees=139.69, elevation_m=40),
    "Nairobi": Topos(latitude_degrees=-1.29,  longitude_degrees=36.82,  elevation_m=1700),
    "Sydney":  Topos(latitude_degrees=-33.87, longitude_degrees=151.21, elevation_m=58),
}
raw_iot_clusters = {
    "Abisko":         {"latitude": 68.35,  "longitude": 18.79,  "elevation_m": 420},
    "Boreal":         {"latitude": 55.50,  "longitude": 105.00, "elevation_m": 450},
    "Taiga":          {"latitude": 58.00,  "longitude": 99.00,  "elevation_m": 300},
    "Patagonia":      {"latitude": 51.00,  "longitude": 73.00,  "elevation_m": 500},
    "Amazon_Forest":  {"latitude": -3.47,  "longitude": -62.37, "elevation_m": 100},
    "Great_Barrier":  {"latitude": -18.29, "longitude": 147.77, "elevation_m": 0},
    "Mediterranean":  {"latitude": 37.98,  "longitude": 23.73,  "elevation_m": 170},
    "California":     {"latitude": 36.78,  "longitude": -119.42,"elevation_m": 150},
}
iot_clusters = {
    name: Topos(latitude_degrees=cfg["latitude"], longitude_degrees=cfg["longitude"], elevation_m=cfg["elevation_m"])
    for name, cfg in raw_iot_clusters.items()
}

current_observer_name = "Berlin"
observer = observer_locations[current_observer_name]

ts = load.timescale()
sim_time = datetime(2025, 3, 30, 0, 0, 0)  # ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘ ì‹œê°„
threshold_deg = 40
sim_paused = False
auto_resume_delay_sec = 0
sim_delta_sec = 10.0        # ì‹œë®¬ë ˆì´ì…˜ í•œ ìŠ¤í… ì¦ê°€ëŸ‰(ì´ˆ)
real_interval_sec = 0.01   # ì‹¤ì œ ë£¨í”„ ìŠ¬ë¦½(ì´ˆ)


def to_ts(dt: datetime):
    """datetime -> skyfield ts.utc(...)"""
    return ts.utc(dt.year, dt.month, dt.day, dt.hour, dt.minute, dt.second)

def get_current_time_utc():
    return to_ts(sim_time)

def elevation_deg(sat: EarthSatellite, topox: Topos, t_ts):
    """ìœ„ì„±-ê´€ì¸¡ì  ê³ ë„(deg) ê³„ì‚°."""
    alt, _, _ = (sat - topox).at(t_ts).altaz()
    return alt.degrees


# ==================== ì—°í•©í•™ìŠµ(FL) ëŸ°íƒ€ì„ ìƒíƒœ ====================
# GPU ê°œìˆ˜ ìë™/í™˜ê²½ ì§€ì •
NUM_GPUS = int(os.getenv("NUM_GPUS", "0"))
if NUM_GPUS == 0 and _has_torch and torch.cuda.is_available():
    NUM_GPUS = torch.cuda.device_count()

# GPUë‹¹ ë™ì‹œ ì„¸ì…˜ ìˆ˜(ê¸°ë³¸ 1)
SESSIONS_PER_GPU = int(os.getenv("SESSIONS_PER_GPU", "10"))

# ì´ ë™ì‹œ í•™ìŠµ ì‘ì—… ìˆ˜
MAX_TRAIN_WORKERS = int(os.getenv(
    "FL_MAX_WORKERS",
    str(max(1, (NUM_GPUS or 1) * max(1, SESSIONS_PER_GPU))))
)

training_executor = ThreadPoolExecutor(max_workers=MAX_TRAIN_WORKERS)
uploader_executor = ThreadPoolExecutor(max_workers=4)  # ì—…ë¡œë“œ/ì§‘ê³„ëŠ” ì§§ê²Œ

train_queue: "asyncio.Queue[int]" = asyncio.Queue(
    maxsize=int(os.getenv("FL_QUEUE_MAX", "1000"))
)

@dataclass
class TrainState:
    running: bool = False
    future: Optional[Future] = None
    gpu_id: Optional[int] = None
    stop_event: threading.Event = field(default_factory=threading.Event)
    last_ckpt_path: Optional[str] = None
    round_idx: int = 0  # Falseâ†’Trueë§ˆë‹¤ ë¼ìš´ë“œ ì¦ê°€
    in_queue: bool = False

train_states: Dict[int, TrainState] = {}

def _save_last_global_ptr(path: Path, version: int):
    """last_global.jsonì„ ì›ìì ìœ¼ë¡œ ê°±ì‹ """
    try:
        tmp = LAST_GLOBAL_PTR.with_suffix(".json.tmp")
        with tmp.open("w", encoding="utf-8") as f:
            json.dump({"path": str(path), "version": int(version)}, f)
        tmp.replace(LAST_GLOBAL_PTR)  # ì›ìì  êµì²´
    except Exception as e:
        logger.warning(f"[AGG] write last_global.json failed: {e}")

def _load_last_global_ptr() -> tuple[int, Optional[Path]]:
    """í¬ì¸í„° íŒŒì¼ì—ì„œ (version, path) ë¡œë“œ. ì—†ìœ¼ë©´ (-1, None)."""
    try:
        if LAST_GLOBAL_PTR.exists():
            with LAST_GLOBAL_PTR.open("r", encoding="utf-8") as f:
                d = json.load(f)
            p = Path(d.get("path", ""))
            v = int(d.get("version", -1))
            if p.exists():
                return v, p
    except Exception as e:
        logger.warning(f"[AGG] read last_global.json failed: {e}")
    return -1, None

def _find_latest_global_ckpt():
    """
    ckpt/global_v*.ckpt ì¤‘ ê°€ì¥ ìµœì‹ (ë²„ì „ ìš°ì„ , ë™ì¼ ë²„ì „ì´ë©´ íƒ€ì„ìŠ¤íƒ¬í”„ ìµœê·¼)ì„ ë°˜í™˜.
    ì˜ˆ) global_v12.ckpt, global_v12_20250813_150128.ckpt ëª¨ë‘ ì§€ì›
    """
    paths = list(CKPT_DIR.glob("global_v*.ckpt"))
    best_ver, best_ts, best_path = -1, "", None
    for p in paths:
        m = re.match(r"global_v(\d+)(?:_(\d{8}_\d{6}))?\.ckpt$", p.name)
        if not m:
            continue
        v = int(m.group(1))
        ts = m.group(2) or ""  # íƒ€ì„ìŠ¤íƒ¬í”„ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´
        if (v > best_ver) or (v == best_ver and ts > best_ts):
            best_ver, best_ts, best_path = v, ts, p
    return best_ver, best_path

def _local_ptr_path(sat_id: int) -> Path:
    return CKPT_DIR / f"sat{sat_id}_last.json"

def _save_last_local_ptr(sat_id: int, path: str, *, from_gver: Optional[int] = None, round_idx: Optional[int] = None, epoch: Optional[int] = None):
    """sat{ID}_last.jsonì„ ì›ìì ìœ¼ë¡œ ê°±ì‹ """
    try:
        p = _local_ptr_path(sat_id)
        tmp = p.with_suffix(".json.tmp")
        meta = {
            "path": str(path),
            "from_gver": int(from_gver) if from_gver is not None else (_parse_fromg(os.path.basename(path)) or -1),
            "round_idx": int(round_idx) if round_idx is not None else None,
            "epoch": int(epoch) if epoch is not None else None,
            "updated_at": datetime.now().isoformat(),
        }
        with tmp.open("w", encoding="utf-8") as f:
            json.dump(meta, f)
        tmp.replace(p)
    except Exception as e:
        logger.warning(f"[LOCAL] write last ptr failed (sat{sat_id}): {e}")

def _load_last_local_ptr(sat_id: int) -> Optional[Dict[str, Any]]:
    """í¬ì¸í„°ì—ì„œ ë©”íƒ€ ë¡œë“œ. ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ None."""
    try:
        p = _local_ptr_path(sat_id)
        if not p.exists():
            return None
        with p.open("r", encoding="utf-8") as f:
            meta = json.load(f)
        path = meta.get("path")
        if path and os.path.exists(path):
            return meta
    except Exception as e:
        logger.warning(f"[LOCAL] read last ptr failed (sat{sat_id}): {e}")
    return None

def _enqueue_training(sat_id: int):
    st = train_states[sat_id]
    if st.running or st.in_queue:
        return
    try:
        train_queue.put_nowait(sat_id)
        st.in_queue = True
        # _log(f"SAT{sat_id}: queued")  # í•„ìš”í•˜ë©´ ì£¼ì„ í•´ì œ
    except asyncio.QueueFull:
        # ë„ˆë¬´ ë§ì€ ìš”ì²­ì´ë©´ ì¡°ìš©íˆ ë“œë¡­(ë˜ëŠ” ë“œë¬¼ê²Œë§Œ ë¡œê·¸)
        pass

def _build_worker_gpu_list():
    # ì˜ˆ: NUM_GPUS=2, SESSIONS_PER_GPU=2 -> [0,0,1,1]
    if NUM_GPUS > 0:
        return [gid for gid in range(NUM_GPUS) for _ in range(SESSIONS_PER_GPU)]
    # GPU ì—†ì„ ë•Œë„ ì›Œì»¤ëŠ” í•„ìš”(=CPU í•™ìŠµ)
    return [None] * max(1, SESSIONS_PER_GPU)

async def _train_worker(gpu_id: Optional[int]):
    loop = asyncio.get_running_loop()
    while True:
        sat_id = await train_queue.get()
        st = train_states[sat_id]
        st.in_queue = False

        # ì´ë¯¸ ë‹¤ë¥¸ ê³³ì—ì„œ ì‹œì‘ë˜ì—ˆìœ¼ë©´ skip
        if st.running:
            train_queue.task_done()
            continue

        st.stop_event.clear()
        st.gpu_id = gpu_id
        st.running = True

        def _job():
            # do_local_training ì•ˆì—ì„œ gpu_idë¡œ ë””ë°”ì´ìŠ¤ ì„ íƒ
            return do_local_training(sat_id=sat_id, stop_event=st.stop_event, gpu_id=gpu_id)

        # ìŠ¤ë ˆë“œí’€ì—ì„œ í•™ìŠµ ì‹¤í–‰ (ì´ë²¤íŠ¸ë£¨í”„ ë¸”ë¡œí‚¹ ë°©ì§€)
        fut = loop.run_in_executor(training_executor, _job)
        st.future = fut  # asyncio.Future

        try:
            ckpt = await fut
        except Exception as e:
            _log(f"SAT{sat_id}: training ERROR: {e}")
            ckpt = None
        finally:
            st.last_ckpt_path = ckpt
            st.running = False
            _log(f"SAT{sat_id}: training DONE (worker gpu={gpu_id}), ckpt={ckpt}")
            train_queue.task_done()

def _upload_and_aggregate_async(sat_id: int, ckpt_path: Optional[str]):
    """ì—…ë¡œë“œ/ì§‘ê³„ ë¹„ë™ê¸° ì‹¤í–‰(ì‚¬ìš©ì í›… í˜¸ì¶œ)."""
    if not ckpt_path:
        _log(f"SAT{sat_id}: no checkpoint to upload")
        return

    def _job():
        _log(f"SAT{sat_id}: uploading {ckpt_path}")
        try:
            if "upload_model_to_server" in globals():
                upload_model_to_server(sat_id=sat_id, ckpt_path=ckpt_path)
                _log(f"SAT{sat_id}: uploaded")
            if "upload_and_aggregate" in globals():
                new_global = upload_and_aggregate(sat_id, ckpt_path)
                _log(f"SAT{sat_id}: aggregated -> {new_global}")
        except Exception as e:
            _log(f"SAT{sat_id}: upload/aggregate ERROR: {e}")

    uploader_executor.submit(_job)

def _on_become_visible(sat_id: int):
    st = train_states[sat_id]
    if st.running and st.future:
        st.stop_event.set()
        def _cb(_fut):
            try:
                _fut.result()  # ì˜ˆì™¸ ì „íŒŒ/ì†Œê±°
            except Exception:
                pass
            _upload_and_aggregate_async(sat_id, train_states[sat_id].last_ckpt_path)
        # asyncio.Future / concurrent.futures.Future ë‘˜ ë‹¤ ì§€ì›
        try:
            st.future.add_done_callback(_cb)
        except Exception:
            # ì¼ë¶€ êµ¬í˜„ì—ì„œ add_done_callback ì‹œê·¸ë‹ˆì²˜ ì°¨ì´ ì²˜ë¦¬
            pass
    else:
        _upload_and_aggregate_async(sat_id, st.last_ckpt_path)

# ---------- Metrics helpers ----------
def _append_csv_row(path: Path, header: List[str], row: List[Any]):
    path.parent.mkdir(parents=True, exist_ok=True)
    new_file = not path.exists()
    with METRICS_LOCK:
        with path.open("a", encoding="utf-8") as f:
            if new_file:
                f.write(",".join(header) + "\n")
            # ê°„ë‹¨ CSV escape
            def esc(x):
                s = "" if x is None else str(x)
                if ("," in s) or ("\"" in s):
                    s = "\"" + s.replace("\"", "\"\"") + "\""
                return s
            f.write(",".join(esc(x) for x in row) + "\n")

def _append_excel_row(xlsx_path: Path, header: List[str], row: List[Any], sheet: str = "metrics"):
    if not _has_pandas:
        return
    xlsx_path.parent.mkdir(parents=True, exist_ok=True)
    import pandas as pd
    try:
        if xlsx_path.exists():
            df = pd.read_excel(xlsx_path, sheet_name=sheet)
        else:
            df = pd.DataFrame(columns=header)
        s = pd.Series(row, index=header)
        df = pd.concat([df, s.to_frame().T], ignore_index=True)
        with pd.ExcelWriter(xlsx_path, engine="openpyxl", mode="w") as w:
            df.to_excel(w, sheet_name=sheet, index=False)
    except Exception as e:
        logger.warning(f"[METRICS] Excel write failed: {e}")

def _log_local_metrics(sat_id: int, round_idx: int, epoch: int, loss: float, acc: float, n: int, ckpt_path: str):
    ts = datetime.now().isoformat()
    header = ["timestamp", "sat_id", "round", "epoch", "num_samples", "loss", "acc", "ckpt_path"]
    row = [ts, sat_id, round_idx, epoch, n, f"{loss:.6f}", f"{acc:.2f}", ckpt_path]
    csv_path = LOCAL_METRICS_DIR / f"sat{sat_id}.csv"
    _append_csv_row(csv_path, header, row)
    _append_excel_row(csv_path.with_suffix(".xlsx"), header, row, sheet="local")

def _log_global_metrics(
    version: int,
    split: str,
    loss: float,
    acc: float,
    n: int,
    ckpt_path: str,
    *,
    f1_macro: float = None,
    madds_M: float = None,
    flops_M: float = None,
    latency_ms: float = None,
):
    """ê¸€ë¡œë²Œ ë©”íŠ¸ë¦­ì„ ë‹¨ì¼ CSV/XLSXë¡œ ëˆ„ì  ì €ì¥."""
    ts = datetime.now().isoformat()
    row = [
        ts,
        version,
        split,
        n,
        f"{loss:.6f}",
        f"{acc:.2f}",
        (f"{f1_macro:.2f}" if f1_macro is not None else ""),
        (f"{madds_M:.2f}"  if madds_M is not None  else ""),
        (f"{flops_M:.2f}"  if flops_M is not None  else ""),
        (f"{latency_ms:.3f}" if latency_ms is not None else ""),
        ckpt_path,
    ]

    # ë‹¨ì¼ CSVë¡œë§Œ ì €ì¥(ë²„ì „ë³„ íŒŒì¼ ìƒì„± X)
    _append_csv_row(GLOBAL_METRICS_CSV, GLOBAL_METRICS_HEADER, row)

    # ë‹¨ì¼ XLSXë„ í•¨ê»˜ ìœ ì§€(ìˆìœ¼ë©´)
    try:
        _append_excel_row(GLOBAL_METRICS_XLSX, GLOBAL_METRICS_HEADER, row, sheet="global")
    finally:
        _log(f"[METRICS] global v{version} {split} saved -> {GLOBAL_METRICS_CSV}")


def _client_num_samples(sat_id: int) -> int:
    try:
        ds = get_training_dataset(sat_id)
        return len(ds) if hasattr(ds, "__len__") else 1
    except Exception:
        return 1
    
def _alpha_for(sat_id: int, s: int) -> float:
    # staleness decay
    decay = _staleness_factor(s, STALENESS_TAU, STALENESS_MODE)
    # size-aware scaling (ë¡œì»¬ ìƒ˜í”Œìˆ˜ê°€ í‰ê· ë³´ë‹¤ í¬ë©´ ì¡°ê¸ˆ ë” í¬ê²Œ)
    try:
        mean_n = sum(_client_num_samples(sid) for sid in satellites) / max(1, len(satellites))
    except Exception:
        mean_n = _client_num_samples(sat_id)
    scale = _client_num_samples(sat_id) / max(1e-9, mean_n)

    alpha_eff = max(W_MIN, AGG_ALPHA * decay * scale)
    alpha_eff = min(alpha_eff, ALPHA_MAX)
    return alpha_eff

def _get_eval_dataset(split: str):
    """
    split in {"val","test"}.
    1) DATA_REGISTRY.get_{split}_dataset() ì‹œë„
    2) data ëª¨ë“ˆ í•¨ìˆ˜(get_validation_dataset / get_test_dataset) ì‹œë„
    3) torchvision CIFAR-10(test) í´ë°±
    """
    # 1) registry method
    meth_name = f"get_{'validation' if split=='val' else 'test'}_dataset"
    if hasattr(DATA_REGISTRY, meth_name):
        try:
            ds = getattr(DATA_REGISTRY, meth_name)()
            if ds is not None:
                _log(f"[EVAL] using DATA_REGISTRY.{meth_name}()")
                return ds
        except Exception as e:
            logger.warning(f"[EVAL] DATA_REGISTRY.{meth_name} failed: {e}")

    # 2) data module fallbacks
    try:
        data_mod = importlib.import_module("data")
        fn_name = "get_validation_dataset" if split == "val" else "get_test_dataset"
        if hasattr(data_mod, fn_name):
            ds = getattr(data_mod, fn_name)()
            if ds is not None:
                _log(f"[EVAL] using data.{fn_name}()")
                return ds
    except Exception as e:
        logger.warning(f"[EVAL] data.{fn_name} failed: {e}")


    # 3) torchvision CIFAR-10 test í´ë°±
    try:
        from torchvision.datasets import CIFAR10
        from torchvision import transforms

        img_size = int(os.getenv("FL_IMG_SIZE", "32"))  # í›ˆë ¨ì´ 224ë¡œ ë¦¬ì‚¬ì´ì¦ˆë©´ 224ë¡œ ì§€ì •
        mean = tuple(map(float, os.getenv("FL_NORM_MEAN", "0.4914,0.4822,0.4465").split(",")))
        std  = tuple(map(float, os.getenv("FL_NORM_STD",  "0.2470,0.2435,0.2616").split(",")))
        tfm = transforms.Compose([
            transforms.Resize(img_size),
            transforms.ToTensor(),
            transforms.Normalize(mean, std),
        ])
        ds = CIFAR10(root=str(CIFAR_ROOT), train=False, download=True, transform=tfm)
        _log(f"[EVAL] using torchvision CIFAR10(test) fallback | size={img_size}, norm=mean{mean},std{std}")
        return ds
    except Exception as e:
        logger.warning(f"[EVAL] fallback CIFAR10 failed: {e}")

    return None

def _evaluate_state_dict(state_dict: dict, dataset, batch_size: int = 512, device: str = "cpu"):
    """
    state_dictë¥¼ ì£¼ì–´ì§„ dataset(TensorDataset ì˜ˆìƒ) ìœ„ì—ì„œ í‰ê°€(loss, acc).
    ê¸°ë³¸ì€ CPUì—ì„œ í‰ê°€(ì•ˆì „).
    """
    import torch
    import torch.nn as nn
    from torch.utils.data import DataLoader
    from torchvision.models import mobilenet_v3_small

    num_classes = int(os.getenv("FL_NUM_CLASSES", "10"))

    model = mobilenet_v3_small(num_classes=num_classes)
    model.load_state_dict(state_dict)
    model.to(device).eval()

    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)
    criterion = nn.CrossEntropyLoss(reduction="sum")
    total_loss, total_correct, total = 0.0, 0, 0

    conf = torch.zeros((num_classes, num_classes), dtype=torch.int64)

    with torch.no_grad():
        for x, y in loader:
            x = x.to(device)
            y = y.to(device, dtype=torch.long)  # â† ì¤‘ìš”
            logits = model(x)
            loss = criterion(logits, y)

            total_loss += float(loss.item())
            pred = logits.argmax(1)
            total_correct += int((pred == y).sum().item())
            total += y.size(0)

            # í˜¼ë™í–‰ë ¬(ë²¡í„°í™”)
            for c in range(num_classes):
                mask = (y == c)
                if mask.any():
                    binc = torch.bincount(pred[mask].cpu(), minlength=num_classes)
                    conf[c] += binc.to(conf.dtype)

    avg_loss = total_loss / max(1, total)
    acc = 100.0 * total_correct / max(1, total)

    tp = torch.diag(conf).to(torch.float32)
    fp = conf.sum(0).to(torch.float32) - tp
    fn = conf.sum(1).to(torch.float32) - tp
    precision = tp / (tp + fp + 1e-12)
    recall    = tp / (tp + fn + 1e-12)
    f1_macro  = float((2 * precision * recall / (precision + recall + 1e-12)).mean().item() * 100.0)

    # --- Latency (bs=1) ---
    # ì…ë ¥ í¬ê¸° ì¶”ì • (dataset ì²« ìƒ˜í”Œ)
    try:
        sample_shape = dataset[0][0].shape  # (C,H,W)
    except Exception:
        sample_shape = (3, 32, 32)  # CIFAR í´ë°±


    x1 = torch.randn(1, *sample_shape, device=device)
    model.eval()
    with torch.no_grad():
        # warmup
        for _ in range(10):
            _ = model(x1)
        if isinstance(device, torch.device) and device.type == "cuda":
            torch.cuda.synchronize(device)
        t0 = time.perf_counter()
        for _ in range(50):
            _ = model(x1)
        if isinstance(device, torch.device) and device.type == "cuda":
            torch.cuda.synchronize(device)
        latency_ms = (time.perf_counter() - t0) / 50.0 * 1000.0

    # --- MAdds / FLOPs (ì„ íƒ) ---
    madds_M, flops_M = None, None
    try:
        # THOP ìš°ì„  ì‹œë„
        from thop import profile
        macs, _params = profile(model, inputs=(x1,), verbose=False)
        madds_M = macs / 1e6
        flops_M = (2 * macs) / 1e6  # multiply+addë¥¼ ê°ê° 1 FLOPìœ¼ë¡œ ê°„ì£¼
    except Exception:
        # fvcore ëŒ€ì•ˆ(ì„¤ì¹˜ë¼ ìˆë‹¤ë©´ ìë™ ì‚¬ìš©)
        try:
            from fvcore.nn import FlopCountAnalysis
            model_cpu = model.to("cpu")
            x_cpu = x1.detach().to("cpu")
            flop_an = FlopCountAnalysis(model_cpu, x_cpu)
            flops = float(flop_an.total())
            flops_M = flops / 1e6
            madds_M = flops_M / 2.0
            model.to(device)
        except Exception:
            pass  # ë‚¨ê²¨ë‘ê¸°(ë©”íŠ¸ë¦­ ì—†ìŒ)

    return avg_loss, acc, total, f1_macro, madds_M, flops_M, float(latency_ms)

# -------------------- ê¸€ë¡œë²Œ ì´ˆê¸°í™”/ì§‘ê³„ --------------------
def _init_global_model():
    """ì„œë²„ ê¸°ë™ ì‹œ ê¸€ë¡œë²Œ ê°€ì¤‘ì¹˜ ë¡œë“œ/ì´ˆê¸°í™”."""
    global GLOBAL_MODEL_STATE, GLOBAL_MODEL_VERSION
    import torch
    with GLOBAL_MODEL_LOCK:
        # 1) í¬ì¸í„° íŒŒì¼ ìš°ì„ 
        ver, path = _load_last_global_ptr()
        # 2) ì—†ìœ¼ë©´ ë””ë ‰í† ë¦¬ ìŠ¤ìº”
        if path is None:
            ver, path = _find_latest_global_ckpt()
            
        if path and path.exists() and ver >= 0:
            GLOBAL_MODEL_STATE = torch.load(path, map_location="cpu")
            GLOBAL_MODEL_VERSION = ver
            print(f"[AGG] Loaded global model v{GLOBAL_MODEL_VERSION} from {path}")
        else:
            # ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±í•´ì„œ v0 ì €ì¥
            model = _new_model_skeleton()
            GLOBAL_MODEL_STATE = model.state_dict()
            GLOBAL_MODEL_VERSION = 0
            ts = datetime.now().strftime("%Y%m%d_%H%M%S")
            init_path = CKPT_DIR / f"global_v0_{ts}.ckpt"
            link_path = CKPT_DIR / "global_v0.ckpt"
            torch.save(GLOBAL_MODEL_STATE, init_path)
            try:
                torch.save(GLOBAL_MODEL_STATE, link_path)
            except Exception:
                pass
            _save_last_global_ptr(init_path, 0)
            print(f"[AGG] Initialized new global model at {init_path}")

def _new_model_skeleton():
    """í•™ìŠµê³¼ ë™ì¼í•œ ì•„í‚¤í…ì²˜ë¡œ ë¹ˆ ëª¨ë¸ ìƒì„±."""
    from torchvision.models import mobilenet_v3_small
    num_classes = int(os.getenv("FL_NUM_CLASSES", "10"))
    model = mobilenet_v3_small(num_classes=num_classes)
    return model

def _find_latest_global_ckpt():
    """ckpt/global_v*.ckpt ì¤‘ ê°€ì¥ ìµœì‹  ë²„ì „ì„ ì°¾ì•„ ë°˜í™˜."""
    import re
    paths = sorted(CKPT_DIR.glob("global_v*.ckpt"))
    best = None
    best_ver = -1
    for p in paths:
        m = re.search(r"global_v(\d+)\.ckpt$", p.name)
        if m:
            v = int(m.group(1))
            if v > best_ver:
                best_ver, best = v, p
    return best_ver, best

def _bn_recalibrate(state_dict: dict, dataset, batches: int = 20, bs: int = 256) -> dict:
    import torch
    from torch.utils.data import DataLoader
    from torchvision.models import mobilenet_v3_small
    model = mobilenet_v3_small(num_classes=int(os.getenv("FL_NUM_CLASSES","10")))
    model.load_state_dict(state_dict, strict=True)
    model.train()  # BN í†µê³„ ì—…ë°ì´íŠ¸ ëª¨ë“œ
    loader = DataLoader(dataset, batch_size=bs, shuffle=True, num_workers=0)
    with torch.no_grad():
        for i, (x, _) in enumerate(loader):
            model(x)
            if i+1 >= batches: break
    model.eval()
    return {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}


def collect_bn_keys(model: nn.Module):
    bn_keys = set()
    for name, m in model.named_modules():
        if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
            for suffix in ("weight", "bias", "running_mean", "running_var", "num_batches_tracked"):
                k = f"{name}.{suffix}" if name else suffix
                if k in model.state_dict():
                    bn_keys.add(k)
    return bn_keys

def load_partial_skip(model: nn.Module, src_sd: dict, skip_keys: set):
    dst = model.state_dict()
    to_load = {k: v for k, v in src_sd.items() if k in dst and k not in skip_keys and v.shape == dst[k].shape}
    missing = [k for k in dst.keys() if k not in to_load and k not in skip_keys]
    model.load_state_dict({**dst, **to_load})
    return to_load.keys(), missing

# ===== server: FedAvg with BN-skip + key-intersection =====
def fedavg_weighted(client_sds, client_weights, skip_keys: set):
    keys = set.intersection(*(set(sd.keys()) for sd in client_sds))
    keys = {k for k in keys if k not in skip_keys}  # BN ì œì™¸
    wsum = float(sum(client_weights))
    out = {}
    for k in keys:
        t0 = client_sds[0][k]
        if torch.is_floating_point(t0):
            acc = None
            for w, sd in zip(client_weights, client_sds):
                v = sd[k].to(dtype=torch.float32)
                acc = (w * v) if acc is None else acc + (w * v)
            out[k] = (acc / wsum).to(t0.dtype)
        else:
            out[k] = t0  # ë¹„ë¶€ë™(ì •ìˆ˜) ë²„í¼ëŠ” ì„ì˜ ë³µì‚¬
    return out

def aggregate_params(global_state: dict, local_state: dict, alpha: float) -> dict:
    """
    ë‹¨ìˆœ ê°€ì¤‘í•© ì§‘ê³„:
      new = (1-alpha)*global + alpha*local
    í‚¤/shape ë¶ˆì¼ì¹˜ í•­ëª©ì€ ê¸€ë¡œë²Œ ê°’ ìœ ì§€.
    """
    new_params = {}
    for k, g_t in global_state.items():
        l_t = local_state.get(k)
        if l_t is None or g_t.shape != l_t.shape:
            new_params[k] = g_t.clone(); continue

        # --- BN running stats ì²˜ë¦¬ ---
        if k.endswith("running_mean") or k.endswith("running_var"):
            mode = os.getenv("FL_AGG_BN_MODE", "blend")
            if mode == "blend" and g_t.is_floating_point() and l_t.is_floating_point():
                g = g_t.detach().to("cpu", dtype=torch.float32)
                l = l_t.detach().to("cpu", dtype=torch.float32)
                alpha_bn = float(os.getenv("FL_AGG_BN_SCALE", "0.1")) * alpha
                new_params[k] = (1.0 - alpha_bn) * g + alpha_bn * l
            else:
                new_params[k] = g_t.clone()
            continue

        if k.endswith("num_batches_tracked"):
            new_params[k] = g_t.clone(); continue

        if not g_t.is_floating_point() or not l_t.is_floating_point():
            new_params[k] = g_t.clone(); continue

        g = g_t.detach().to("cpu", dtype=torch.float32)
        l = l_t.detach().to("cpu", dtype=torch.float32)
        new_params[k] = (1.0 - alpha) * g + alpha * l
    return new_params

def _parse_fromg(ckpt_path: str) -> int | None:
    m = _fromg_re.search(os.path.basename(ckpt_path))
    return int(m.group(1)) if m else None
    
def _staleness_factor(s: int, tau: float, mode: str) -> float:
    if tau <= 0: return 1.0
    if mode == "poly":
        return (1.0 + s) ** (-tau)
    # default exp
    return math.exp(-float(s) / tau)

def upload_and_aggregate(sat_id: int, ckpt_path: str) -> str:
    """
    ìœ„ì„±ì—ì„œ ì˜¬ë¼ì˜¨ ë¡œì»¬ ckpt(=state_dict)ë¥¼ ê¸€ë¡œë²Œì— í•©ì¹˜ê³ ,
    ìƒˆë¡œìš´ ê¸€ë¡œë²Œ ckpt ê²½ë¡œë¥¼ ë°˜í™˜.
    """
    import torch
    import datetime as _dt
    if not ckpt_path or not os.path.exists(ckpt_path):
        raise FileNotFoundError(f"ckpt not found: {ckpt_path}")

    local_state = torch.load(ckpt_path, map_location="cpu")

    with GLOBAL_MODEL_LOCK:
        global GLOBAL_MODEL_STATE, GLOBAL_MODEL_VERSION

        base_ver = _parse_fromg(ckpt_path)
        s = max(0, GLOBAL_MODEL_VERSION - (base_ver or GLOBAL_MODEL_VERSION))
        if S_MAX_DROP > 0 and s > S_MAX_DROP:
            _log(f"[AGG] drop stale update: s={s} (> {S_MAX_DROP}) from {ckpt_path}")
            return str(CKPT_DIR / f"global_v{GLOBAL_MODEL_VERSION}.ckpt")
        
        # decay = _staleness_factor(s, STALENESS_TAU, STALENESS_MODE)
        alpha_eff = _alpha_for(sat_id, s)
        
        GLOBAL_MODEL_STATE = aggregate_params(GLOBAL_MODEL_STATE, local_state, alpha_eff)
        GLOBAL_MODEL_VERSION += 1

        ts = _dt.datetime.now().strftime("%Y%m%d_%H%M%S")
        out_path = CKPT_DIR / f"global_v{GLOBAL_MODEL_VERSION}_{ts}.ckpt"
        torch.save(GLOBAL_MODEL_STATE, out_path)
        # canonical link-like íŒŒì¼
        link_path = CKPT_DIR / f"global_v{GLOBAL_MODEL_VERSION}.ckpt"
        try:
            torch.save(GLOBAL_MODEL_STATE, link_path)
        except Exception:
            pass

        _save_last_global_ptr(out_path, GLOBAL_MODEL_VERSION)

        _log(f"[AGG] merged s={s}, tau={STALENESS_TAU}, mode={STALENESS_MODE}, "
            f"alpha_eff={alpha_eff:.4f} (base_gv={base_ver})")

    # ---- ê¸€ë¡œë²Œ í‰ê°€ ë° ë©”íŠ¸ë¦­ ë¡œê¹… (ì£¼ê¸° EVAL_EVERY_N) ----
    try:
        if GLOBAL_MODEL_VERSION % max(1, EVAL_EVERY_N) == 0:
            ds_val  = _get_eval_dataset("val")
            ds_test = _get_eval_dataset("test")

            calib_ds = ds_val or ds_test
            if calib_ds is None:
                _log(f"[AGG] Global v{GLOBAL_MODEL_VERSION}: skipped eval (no dataset)")

            else:
                # 1) BN ì¬ë³´ì • 1íšŒ (val ìš°ì„ , ì—†ìœ¼ë©´ test)
                GLOBAL_MODEL_STATE = _bn_recalibrate(
                    GLOBAL_MODEL_STATE, calib_ds,
                    batches=int(os.getenv("FL_BN_CALIB_BATCHES","20")),
                    bs=int(os.getenv("FL_EVAL_BS","256"))
                )
                # 2) ì¬ë³´ì • ìƒíƒœë¥¼ ê°™ì€ ckptì— ë°˜ì˜ (ì¬í˜„ì„±/ì‹œì‘ì  ì¼ì¹˜)
                try:
                    torch.save(GLOBAL_MODEL_STATE, out_path)
                    torch.save(GLOBAL_MODEL_STATE, link_path)
                except Exception:
                    pass

                _save_last_global_ptr(out_path, GLOBAL_MODEL_VERSION)

                # 3) split ë³„ ë³„ë„ í‰ê°€ (ì¬ë³´ì •ì€ ìœ ì§€í•˜ë˜ ì¬ë³´ì • ë°˜ë³µ X)
                def _eval_and_log(split, ds):
                    if ds is None: 
                        _log(f"[AGG] Global v{GLOBAL_MODEL_VERSION} {split}: no dataset")
                        return
                    g_loss, g_acc, n, g_f1, g_madds, g_flops, g_lat = _evaluate_state_dict(
                        GLOBAL_MODEL_STATE, ds, batch_size=EVAL_BS, device="cpu"
                    )
                    _log(f"[AGG] Global v{GLOBAL_MODEL_VERSION} {split}: acc={g_acc:.2f}% loss={g_loss:.4f} (n={n})")
                    _log_global_metrics(
                        GLOBAL_MODEL_VERSION, split, g_loss, g_acc, n, str(out_path),
                        f1_macro=g_f1, madds_M=g_madds, flops_M=g_flops, latency_ms=g_lat
                    )

                _eval_and_log("val",  ds_val)
                _eval_and_log("test", ds_test)
    except Exception as e:
        logger.warning(f"[AGG] global evaluation failed: {e}")

    return str(out_path)

# === ë¡œì»¬ í•™ìŠµ í•¨ìˆ˜: do_local_training ===
def do_local_training(
    sat_id: int,
    stop_event: threading.Event,
    gpu_id: Optional[int] = None,
    *,
    epochs: Optional[int] = None,
    lr: Optional[float] = None,
    batch_size: Optional[int] = None,
) -> str:
    """
    ë¡œì»¬ í•™ìŠµì„ ìˆ˜í–‰í•˜ê³  ë§ˆì§€ë§‰ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œë¥¼ ë°˜í™˜.
    - stop_eventê°€ setë˜ë©´ ê°€ëŠ¥í•œ ë¹ ë¥´ê²Œ ì¤‘ë‹¨.
    - gpu_idê°€ ì£¼ì–´ì§€ë©´ í•´ë‹¹ GPUë¥¼ ì‚¬ìš©.
    - ë°ì´í„°ëŠ” data.get_training_dataset(sat_id)ì—ì„œ íšë“.
    - ê¸€ë¡œë²Œ ì´ˆê¸° ê°€ì¤‘ì¹˜ í›…(get_initial_model_state)ì´ ìˆìœ¼ë©´ ì ìš©.
    """
    from torch.utils.data import DataLoader
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torchvision.models import mobilenet_v3_small

    # ---- í•˜ì´í¼íŒŒë¼ë¯¸í„°: í™˜ê²½ë³€ìˆ˜ -> ì¸ì -> ê¸°ë³¸ê°’ ----
    EPOCHS = int(os.getenv("FL_EPOCHS_PER_ROUND", "10")) if epochs is None else int(epochs)
    LR     = float(os.getenv("FL_LR", "1e-3"))          if lr is None else float(lr)
    BS     = int(os.getenv("FL_BATCH_SIZE", "64"))      if batch_size is None else int(batch_size)
    NUM_CLASSES = int(os.getenv("FL_NUM_CLASSES", "10"))

    # ---- ë””ë°”ì´ìŠ¤ ì„ ì • ----
    if torch.cuda.is_available():
        device = torch.device(f"cuda:{gpu_id}") if gpu_id is not None else torch.device("cuda")
    else:
        device = torch.device("cpu")

    # ---- ëª¨ë¸ ì¤€ë¹„ ----
    model = mobilenet_v3_small(num_classes=NUM_CLASSES)
    start_gver = globals().get("GLOBAL_MODEL_VERSION", -1)  # <- ê¸°ë³¸ê°’
    resumed = False
    try:
        meta = _load_last_local_ptr(sat_id)
        if meta and meta.get("path"):
            sd = torch.load(meta["path"], map_location="cpu")
            model.load_state_dict(sd, strict=False)
            # from_gverì´ í¬ì¸í„°ì— ì—†ìœ¼ë©´ íŒŒì¼ëª…ì—ì„œ íŒŒì‹±
            fg = meta.get("from_gver")
            if fg is None or fg < 0:
                fg = _parse_fromg(os.path.basename(meta["path"])) or start_gver
            start_gver = int(fg)
            resumed = True
            _log(f"SAT{sat_id}: resumed from local ckpt {meta['path']} (from_gv={start_gver})")
    except Exception as e:
        _log(f"SAT{sat_id}: failed to resume local ckpt; fallback to global. err={e}")

    if not resumed and "get_global_model_snapshot" in globals():
        try:
            start_gver, state_dict = get_global_model_snapshot()
            if state_dict is not None:
                model.load_state_dict(state_dict, strict=False)
        except Exception as e:
            _log(f"SAT{sat_id}: failed to load global snapshot v{start_gver}: {e}")
    
    _log(f"SAT{sat_id}: starting local training using global v{start_gver}")
    
    model.to(device)
    model.train()

    # ---- ë°ì´í„° ì¤€ë¹„ (ë°˜ë“œì‹œ data ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì‚¬ìš©) ----
    dataset = get_training_dataset(sat_id)
    pin_mem = torch.cuda.is_available() and DL_WORKERS > 0
    loader = DataLoader(
        dataset,
        batch_size=BS,
        shuffle=True,
        drop_last=False,
        num_workers=DL_WORKERS,
        pin_memory=pin_mem,
        persistent_workers=(DL_WORKERS > 0),
    )

    # ---- ì˜µí‹°ë§ˆì´ì €/ë¡œìŠ¤ ----
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LR)

    def save_ckpt(ep: int) -> str:
        import datetime as _dt
        ts = _dt.datetime.now().strftime("%Y%m%d_%H%M%S")
        fname = f"sat{sat_id}_fromg{start_gver}_round{train_states[sat_id].round_idx}_ep{ep}_{ts}.ckpt"
        ckpt_path = CKPT_DIR / fname
        torch.save(model.state_dict(), ckpt_path)
        # â–¼ í¬ì¸í„° ê°±ì‹ 
        _save_last_local_ptr(
            sat_id,
            str(ckpt_path),
            from_gver=start_gver,
            round_idx=train_states[sat_id].round_idx,
            epoch=ep,
        )
        # â–¼ ë©”ëª¨ë¦¬ ìƒíƒœì—ë„ ìµœì‹  ê²½ë¡œ ë°˜ì˜ (ê°€ì‹œì„± ì „ì´ ì‹œ ì—…ë¡œë“œ ìš©)
        try:
            train_states[sat_id].last_ckpt_path = str(ckpt_path)
        except Exception:
            pass
        return str(ckpt_path)

    last_ckpt = None
    n_total = len(dataset) if hasattr(dataset, "__len__") else None

    for ep in range(EPOCHS):
        if stop_event.is_set():
            break

        running_loss, correct, total = 0.0, 0, 0
        for images, labels in loader:
            if stop_event.is_set():
                break
            images = images.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True)

            optimizer.zero_grad(set_to_none=True)
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += float(loss.item())
            _, pred = outputs.max(1)
            total += labels.size(0)
            correct += int(pred.eq(labels).sum().item())

        avg_loss = running_loss / max(1, len(loader))
        acc = 100.0 * correct / max(1, total)
        last_ckpt = save_ckpt(ep)

        _log(f"SAT{sat_id}: ep={ep+1}/{EPOCHS} loss={avg_loss:.4f} acc={acc:.2f}% from_gv={start_gver} saved={last_ckpt}")

        # ---- ë¡œì»¬ ë©”íŠ¸ë¦­ ë¡œê¹… ----
        try:
            _log_local_metrics(
                sat_id=sat_id,
                round_idx=train_states[sat_id].round_idx,
                epoch=ep,
                loss=avg_loss,
                acc=acc,
                n=n_total or total,
                ckpt_path=last_ckpt
            )
        except Exception as e:
            logger.warning(f"[METRICS] local write failed (sat{sat_id}): {e}")

    return last_ckpt or save_ckpt(-1)
    
def get_global_model_snapshot():
    """(version, deepcopy(state_dict))ë¥¼ ì›ìì ìœ¼ë¡œ ê°€ì ¸ì˜¨ë‹¤."""
    with GLOBAL_MODEL_LOCK:
        ver = GLOBAL_MODEL_VERSION
        state = copy.deepcopy(GLOBAL_MODEL_STATE)
    return ver, state

def consolidate_existing_global_metrics(remove_old: bool = False):
    """
    ê³¼ê±° per-version CSVë“¤ì„ ë‹¨ì¼ global_metrics.csvë¡œ ë³‘í•©.
    - ì´ë¯¸ ë‹¨ì¼ íŒŒì¼ì´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ append(ì¤‘ë³µ ë°©ì§€ X â†’ í•„ìš”ì‹œ ìˆ˜ë™ ì •ë¦¬)
    - older íŒŒì¼ í—¤ë”ê°€ (loss, acc, ckpt_path)ë§Œ ìˆì–´ë„ ìë™ ë§¤í•‘, ë‚˜ë¨¸ì§„ ë¹ˆì¹¸
    - remove_old=Trueë©´ ë³‘í•© í›„ ê¸°ì¡´ íŒŒì¼ ì‚­ì œ
    """
    import csv
    merged = 0
    for p in sorted(GLOBAL_METRICS_DIR.glob("global*.csv")):
        if p.name == GLOBAL_METRICS_CSV.name:
            continue
        try:
            with p.open("r", encoding="utf-8") as f:
                reader = csv.DictReader(f)
                # í–‰ë§ˆë‹¤ ë‹¨ì¼ í—¤ë”ë¡œ ë§¤í•‘
                for r in reader:
                    row = [
                        r.get("timestamp", ""),
                        r.get("version", ""),
                        r.get("split", ""),
                        r.get("num_samples", ""),
                        r.get("loss", ""),
                        r.get("acc", ""),
                        r.get("f1_macro", ""),
                        r.get("madds_M", ""),
                        r.get("flops_M", ""),
                        r.get("latency_ms", ""),
                        r.get("ckpt_path", ""),
                    ]
                    _append_csv_row(GLOBAL_METRICS_CSV, GLOBAL_METRICS_HEADER, row)
                    try:
                        _append_excel_row(GLOBAL_METRICS_XLSX, GLOBAL_METRICS_HEADER, row, sheet="global")
                    except Exception:
                        pass
                    merged += 1
            if remove_old:
                try:
                    p.unlink()
                except Exception:
                    pass
        except Exception as e:
            logger.warning(f"[METRICS] consolidate skip {p}: {e}")
    _log(f"[METRICS] consolidated {merged} rows into {GLOBAL_METRICS_CSV}")

# ==================== FastAPI ì•±/ìˆ˜ëª…ì£¼ê¸° ====================
@asynccontextmanager
async def lifespan(app: FastAPI):
    await initialize_simulation()
    try:
        yield
    finally:
        # executors ì •ë¦¬
        training_executor.shutdown(wait=False, cancel_futures=True)
        uploader_executor.shutdown(wait=False, cancel_futures=True)


app = FastAPI(
    title="Satellite Communication API",
    description="ìœ„ì„±, ì§€ìƒêµ­, IoT í´ëŸ¬ìŠ¤í„° ê°„ í†µì‹  ìƒíƒœ ë° ê´€ì¸¡ ê°€ëŠ¥ ì‹œê°„ ë“±ì„ ì œê³µí•˜ëŠ” API ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_url="/openapi.json",
    lifespan=lifespan,
)


# ==================== ì„œë²„ ê¸°ë™ ì‹œ ì²˜ë¦¬ ====================
async def initialize_simulation():
    global satellites

    tle_path = "../constellation.tle"
    if not os.path.exists(tle_path):
        raise FileNotFoundError(f"TLE íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {tle_path}")

    with open(tle_path, "r") as f:
        lines = [line.strip() for line in f.readlines()]
        for i in range(0, len(lines), 3):
            name, line1, line2 = lines[i:i+3]
            sat_id = int(name.replace("SAT", ""))
            satellite = EarthSatellite(line1, line2, name, ts)
            satellites[sat_id] = satellite

    # ì´ˆê¸° ìœ„ì¹˜ ê³„ì‚° & FL ìƒíƒœ ì´ˆê¸°í™”
    t = get_current_time_utc()
    for sat_id, satellite in satellites.items():
        subpoint = satellite.at(t).subpoint()
        current_sat_positions[sat_id] = {
            "lat": subpoint.latitude.degrees,
            "lon": subpoint.longitude.degrees,
        }
        sat_comm_status[sat_id] = False
        train_states[sat_id] = TrainState()
    
    try:
        meta = _load_last_local_ptr(sat_id)
        if meta and meta.get("path"):
            train_states[sat_id].last_ckpt_path = meta["path"]
            _log(f"[INIT] SAT{sat_id}: restored last local ckpt -> {meta['path']} (from_gv={meta.get('from_gver')})")
    except Exception as e:
        logger.warning(f"[INIT] restore local ptr failed (sat{sat_id}): {e}")

    # --- CIFAR-10 ë¡œë“œ & ìœ„ì„±ë³„ ê°€ìƒ ë°ì´í„°ì…‹ ë°°ì • ---
    try:
        DATA_REGISTRY.load_base(root=CIFAR_ROOT, seed=RNG_SEED, download=True)
        assign_file = ASSIGNMENTS_DIR / f"assign_seed{RNG_SEED}_alpha{DIRICHLET_ALPHA}_spc{SAMPLES_PER_CLIENT}.npz"
        if not DATA_REGISTRY.load_assignments(assign_file):
            sat_ids_sorted = sorted(satellites.keys())
            DATA_REGISTRY.assign_clients(
                sat_ids=sat_ids_sorted,
                samples_per_client=SAMPLES_PER_CLIENT,
                alpha=DIRICHLET_ALPHA,
                seed=RNG_SEED,
                with_replacement=WITH_REPLACEMENT,
            )
            DATA_REGISTRY.save_assignments(assign_file)
        logger.info(f"[DATA] CIFAR ready. per_client={SAMPLES_PER_CLIENT}, alpha={DIRICHLET_ALPHA}, seed={RNG_SEED}")
    except Exception as e:
        logger.error(f"[DATA] CIFAR init failed: {e}")

    _init_global_model()

    for gid in _build_worker_gpu_list():
        asyncio.create_task(_train_worker(gid))

    asyncio.create_task(simulation_loop())


# ==================== ì‹œë®¬ë ˆì´ì…˜ ë£¨í”„ ====================
async def simulation_loop():
    global sim_time, current_sat_positions

    while True:
        if not sim_paused:
            t_ts = get_current_time_utc()
            current_sat_positions = {}

            for sat_id, satellite in satellites.items():
                # ì´ì „ ê°€ì‹œì„±
                prev_visible = bool(sat_comm_status.get(sat_id, False))

                # í˜„ì¬ ê°€ì‹œì„± ê³„ì‚°
                alt_deg = elevation_deg(satellite, observer, t_ts)
                visible_now = (alt_deg >= threshold_deg)
                sat_comm_status[sat_id] = visible_now

                # ìœ„ì¹˜ ì—…ë°ì´íŠ¸
                subpoint = satellite.at(t_ts).subpoint()
                current_sat_positions[sat_id] = {
                    "lat": subpoint.latitude.degrees,
                    "lon": subpoint.longitude.degrees,
                }

                # ì—°í•©í•™ìŠµ íŠ¸ë¦¬ê±°
                if not visible_now:
                    # ì˜¤í”„ë¼ì¸ â†’ í•™ìŠµ ì‹œì‘(ê°€ëŠ¥ ì‹œ)
                    _enqueue_training(sat_id)
                elif (not prev_visible) and visible_now:
                    # False -> True ì „ì´
                    _on_become_visible(sat_id)
                    train_states[sat_id].round_idx += 1

            sim_time += timedelta(seconds=sim_delta_sec)

        await asyncio.sleep(real_interval_sec)


# ==================== ëŒ€ì‹œë³´ë“œ / í˜ì´ì§€ ====================
@app.get("/dashboard", response_class=HTMLResponse, tags=["PAGE"])
def dashboard():
    """
    ëŒ€ì‹œë³´ë“œ HTML í˜ì´ì§€
    """
    paused_status = "Paused" if sim_paused else "Running"
    return f"""
    <html>
    <head>
        <title>Satellite Communication Dashboard</title>
        <meta http-equiv="refresh" content="5">
        <style>
            body {{ font-family: Arial; background: #f2f2f2; margin: 2em; }}
            h1 {{ color: #333; }}
            a {{ display: block; margin: 10px 0; font-size: 1.2em; }}
        </style>
    </head>
    <body>
        <h1>ğŸ›°ï¸ğŸ›° Satellite Communication Dashboard</h1>
        <p><b>Sim Time:</b> {sim_time.isoformat()}</p>
        <p><b>Status:</b> {paused_status}</p>
        <p><b>Step:</b> Î”t={sim_delta_sec}s, Interval={real_interval_sec}s</p>
        <div class="control-form">
            <label>Î”t(ì´ˆ): <input id="delta" type="number" step="any" value="{sim_delta_sec}" /></label>
            <label>ê°„ê²©(ì´ˆ): <input id="interval" type="number" step="any" value="{real_interval_sec}" /></label>
            <button onclick="setStep()">ì ìš©</button>
        </div>
        <script>
        async function setStep() {{
            const d = document.getElementById('delta').value;
            const i = document.getElementById('interval').value;
            const res = await fetch(`/api/set_step?delta_sec=${{d}}&interval_sec=${{i}}`, {{ method: 'PUT' }});
            const data = await res.json();
            if (!data.error) {{
                alert(`ì„¤ì • ì™„ë£Œ: Î”t=${{data.sim_delta_sec}}, Interval=${{data.real_interval_sec}}`);
                window.location.reload();
            }} else {{
                alert(`ì˜¤ë¥˜: ${{data.error}}`);
            }}
        }}
        </script>
        <hr>
        <a href="/gs_visibility">ğŸ›°ï¸ GSë³„ í†µì‹  ê°€ëŠ¥ ìœ„ì„± ë³´ê¸°</a>
        <a href="/orbit_paths/lists">ğŸ›° ìœ„ì„±ë³„ ê¶¤ì  ê²½ë¡œ ë³´ê¸°</a>
        <a href="/map_path">ğŸ—º ì§€ë„ ê¸°ë°˜ ìœ„ì„± ê²½ë¡œ ë³´ê¸°</a>
        <a href="/visibility_schedules/lists">ğŸ“… ìœ„ì„±ë³„ ê´€ì¸¡ ê°€ëŠ¥ ì‹œê°„ëŒ€ ëª©ë¡ ë³´ê¸°</a>
        <a href="/iot_clusters"> ğŸ“¡ IoT í´ëŸ¬ìŠ¤í„°ë³„ ìœ„ì¹˜ ë³´ê¸°</a>
        <a href="/iot_visibility"> ğŸŒ IoT í´ëŸ¬ìŠ¤í„°ë³„ í†µì‹  ê°€ëŠ¥ ìœ„ì„± ë³´ê¸°</a>
        <a href="/comm_targets/lists">ğŸš€ ìœ„ì„± í†µì‹  ëŒ€ìƒ í™•ì¸</a>
    </body>
    </html>
    """


@app.get("/gs_visibility", response_class=HTMLResponse, tags=["PAGE"])
def gs_visibility():
    """
    ì§€ìƒêµ­ë³„ë¡œ ê´€ì¸¡ ê°€ëŠ¥í•œ ìœ„ì„± ëª©ë¡ì„ HTMLë¡œ ë°˜í™˜í•˜ëŠ” í˜ì´ì§€
    """

    paused_status = "Paused" if sim_paused else "Running"
    gs_sections = []
    t_ts = get_current_time_utc()

    for name, gs in observer_locations.items():
        rows = []
        for sid, sat in satellites.items():
            alt_deg = elevation_deg(sat, gs, t_ts)
            if alt_deg >= threshold_deg:
                rows.append(f'<tr><td>{sid}</td><td>{alt_deg:.2f}Â°</td></tr>')
        table_html = f"""
        <h2>{name}</h2>
        <table>
            <tr><th>Sat ID</th><th>Elevation</th></tr>
            {''.join(rows)}
        </table>
        """
        gs_sections.append(table_html)

    return f"""
    <html>
    <head>
        <title>GS Visibility</title>
        <meta http-equiv="refresh" content="5">
        <style>
            body {{ font-family: Arial; background: #f2f2f2; margin: 2em; }}
            h2 {{ margin-top: 2em; }}
            table {{ border-collapse: collapse; width: 60%; margin-bottom: 2em; }}
            th, td {{ border: 1px solid #ccc; padding: 8px; text-align: center; }}
        </style>
    </head>
    <body>
        <p><a href="/dashboard">â† Back to Dashboard</a></p>
        <h1>ğŸ›°ï¸ GS-wise Visible Satellites</h1>
        <p><b>Sim Time:</b> {sim_time.isoformat()}</p>
        <p><b>Status:</b> {paused_status}</p>
        <p><b>Step:</b> Î”t={sim_delta_sec}s, Interval={real_interval_sec}s</p>
        <hr>
        {''.join(gs_sections)}
    </body>
    </html>
    """


@app.get("/orbit_paths/lists", response_class=HTMLResponse, tags=["PAGE"])
def sat_paths():
    """
    ìœ„ì„±ë³„ ê¶¤ì  ê²½ë¡œ ë§í¬ ëª©ë¡ì„ HTMLë¡œ ë°˜í™˜í•˜ëŠ” í˜ì´ì§€
    """
    links = [f'<li><a href="/orbit_paths?sat_id={sid}">SAT{sid} Path</a></li>' for sid in sorted(satellites)]
    return f"""
    <html>
    <head>
        <title>Satellite Paths</title>
        <style>
            body {{ font-family: Arial; margin: 2em; }}
            ul {{ list-style-type: none; padding: 0; }}
            li {{ margin-bottom: 10px; }}
        </style>
    </head>
    <body>
        <p><a href="/dashboard">â† Back to Dashboard</a></p>
        <h1>ğŸ›° All Satellite Orbit Paths</h1>
        <ul>
            {''.join(links)}
        </ul>
    </body>
    </html>
    """


@app.get("/orbit_paths", response_class=HTMLResponse, tags=["PAGE"])
def orbit_paths(sat_id: int = Query(...)):
    """
    íŠ¹ì • ìœ„ì„±ì˜ ê¶¤ì  ê²½ë¡œë¥¼ HTMLë¡œ ë°˜í™˜í•˜ëŠ” í˜ì´ì§€
    """
    if sat_id not in satellites:
        return HTMLResponse(f"<p>Error: sat_id {sat_id} not found</p>", status_code=404)

    satellite = satellites[sat_id]
    t0 = sim_time
    positions = []
    for offset_sec in range(0, 7200, 60):
        future = t0 + timedelta(seconds=offset_sec)
        t_ts = to_ts(future)
        subpoint = satellite.at(t_ts).subpoint()
        positions.append((subpoint.latitude.degrees, subpoint.longitude.degrees))

    rows = ''.join([f'<tr><td>{i*60}s</td><td>{lat:.2f}</td><td>{lon:.2f}</td></tr>'
                    for i, (lat, lon) in enumerate(positions)])

    return f"""
    <html>
    <head>
        <title>Orbit Track for SAT{sat_id}</title>
        <style>
            body {{ font-family: Arial; margin: 2em; }}
            table {{ border-collapse: collapse; width: 60%; }}
            th, td {{ border: 1px solid #ccc; padding: 8px; text-align: center; }}
        </style>
    </head>
    <body>
        <p><a href="/orbit_paths/lists">â† Back to All Satellite Orbit Paths</a></p>
        <h1>ğŸ›° SAT{sat_id} Orbit Path</h1>
        <table>
            <tr><th>Offset</th><th>Latitude</th><th>Longitude</th></tr>
            {rows}
        </table>
    </body>
    </html>
    """


@app.get("/map_path", response_class=HTMLResponse, tags=["PAGE"])
def map_path():
    """
    ì§€ë„ ê¸°ë°˜ ìœ„ì„± ê²½ë¡œë¥¼ í‘œì‹œí•˜ëŠ” HTML í˜ì´ì§€
    """
    options = ''.join(f'<option value="{sid}">SAT{sid}</option>' for sid in sorted(satellites))
    obs_data_json = json.dumps({name: {"lat": gs.latitude.degrees, "lon": gs.longitude.degrees}
                                for name, gs in observer_locations.items()})
    iot_data_json = json.dumps(raw_iot_clusters)
    return f"""
    <html>
    <head>
        <title>Map Path</title>
        <link rel="stylesheet" href="https://unpkg.com/leaflet@1.9.4/dist/leaflet.css" />
        <script src="https://unpkg.com/leaflet@1.9.4/dist/leaflet.js"></script>
        <style>
            #map {{ height: 90vh; }}
            html, body {{ margin: 0; padding: 0; }}
            .coverage-circle {{ fill-opacity:0.5; stroke-width:0; }}
            .leaflet-tooltip.no-box {{
                background: transparent; border: none; box-shadow: none; padding: 0; font-weight: bold;
            }}
            .leaflet-tooltip.no-box::before {{ display: none; }}
        </style>
    </head>
    <body>
        <p><a href="/dashboard">â† Back to Dashboard</a></p>
        <h1>ğŸ—º Satellite Map Path</h1>
        <div id="sim-time"></div>
        <div id="sim-step" style="margin-bottom: 1em;"></div>
        <label for="sat_id">Choose a satellite:</label>
        <select id="sat_id" onchange="drawTrajectory(this.value)">{options}</select>
        <div id="map"></div>
        <script>
            var map = L.map('map', {{
                center: [0, 0],
                zoom: 2,
                worldCopyJump: false,
                maxBounds: [[-85, -180], [85, 180]],
                maxBoundsViscosity: 1.0,
                inertia: false
            }});
            L.tileLayer('https://{{s}}.tile.openstreetmap.org/{{z}}/{{x}}/{{y}}.png').addTo(map);

            const observers = {obs_data_json};
            const iotClusters = {iot_data_json};

            var circles = [];
            var pathLines = [];
            var currentMarker = null;
            var currentLabel = null;
            var markerInterval = null;
            var coverCircles = [];

            function drawCoverage() {{
                for (let [name, loc] of Object.entries(observers)) {{
                    let c = L.circle([loc.lat, loc.lon], {{
                        radius: 100000, color: 'green', fillColor: 'green', className: 'coverage-circle'
                    }}).addTo(map);
                    c.bindTooltip(name, {{ permanent: true, direction: 'center', className: 'no-box' }});
                    coverCircles.push(c);
                }}
                for (let [name, loc] of Object.entries(iotClusters)) {{
                    let c = L.circle([loc.latitude, loc.longitude], {{
                        radius: 50000, color: 'orange', fillColor: 'orange', className: 'coverage-circle'
                    }}).addTo(map);
                    c.bindTooltip(name, {{ permanent: true, direction: 'center', className: 'no-box' }});
                    coverCircles.push(c);
                }}
            }}

            async function drawTrajectory(sat_id) {{
                circles.forEach(c => map.removeLayer(c)); circles = [];
                pathLines.forEach(p => map.removeLayer(p)); pathLines = [];
                if (currentMarker) map.removeLayer(currentMarker);
                if (currentLabel) map.removeLayer(currentLabel);
                if (markerInterval) clearInterval(markerInterval);

                drawCoverage();

                let resp = await fetch(`/api/trajectory?sat_id=${{sat_id}}`);
                let satData = await resp.json();
                for (let segment of satData.segments) {{
                    let latlngs = [];
                    for (let p of segment) {{
                        let latlng = [p.lat, p.lon];
                        latlngs.push(latlng);
                        let marker = L.circleMarker(latlng, {{radius: 1, color: 'red'}}).addTo(map);
                        circles.push(marker);
                    }}
                    let line = L.polyline(latlngs, {{color: 'blue', weight: 1}}).addTo(map);
                    pathLines.push(line);
                }}
                markerInterval = setInterval(async () => {{
                    let live = await fetch(`/api/position?sat_id=${{sat_id}}`);
                    let data = await live.json();
                    let simResp = await fetch(`/api/sim_time`);
                    let simData = await simResp.json();
                    document.getElementById('sim-time').innerHTML = `<p><b>Sim Time:</b> ${{simData.sim_time}}</p>`;
                    document.getElementById('sim-step').innerHTML = `<p><b>Step:</b> Î”t={sim_delta_sec}s, Interval={real_interval_sec}s</p>`;

                    if (data.lat !== undefined && data.lon !== undefined) {{
                        if (currentMarker) map.removeLayer(currentMarker);
                        if (currentLabel) map.removeLayer(currentLabel);
                        currentMarker = L.circleMarker([data.lat, data.lon], {{radius: 3, color: 'blue'}}).addTo(map);
                        currentLabel = L.marker([data.lat, data.lon], {{
                            icon: L.divIcon({{
                                className: 'current-label',
                                html: '<b>í˜„ì¬ ìœ„ì„± ìœ„ì¹˜</b>',
                                iconSize: [120, 20],
                                iconAnchor: [60, -10]
                            }})
                        }}).addTo(map);
                    }}
                }}, 1000);
            }}

            window.onload = () => {{
                const selector = document.getElementById('sat_id');
                if (selector.value) drawTrajectory(selector.value);
            }}
        </script>
    </body>
    </html>
    """

@app.get("/visibility_schedules/lists", response_class=HTMLResponse, tags=["PAGE"])
def get_list_visibility_schedules():
    """
    ìœ„ì„±ë³„ ê´€ì¸¡ ê°€ëŠ¥ ì‹œê°„ëŒ€ ë§í¬ ëª©ë¡ì„ HTMLë¡œ ë°˜í™˜í•˜ëŠ” í˜ì´ì§€
    """
    links = [f'<li><a href="/visibility_schedules?sat_id={sid}">SAT{sid} Schedule</a></li>' for sid in sorted(satellites)]
    return f"""
    <html>
    <head>
        <title>Visibility Schedule List</title>
        <style>
            body {{ font-family: Arial; margin: 2em; }}
            ul {{ list-style-type: none; padding: 0; }}
            li {{ margin-bottom: 10px; }}
        </style>
    </head>
    <body>
        <p><a href="/dashboard">â† Back to Dashboard</a></p>
        <h1>ğŸ“… All Satellite Visibility Schedules</h1>
        <ul>
            {''.join(links)}
        </ul>
    </body>
    </html>
    """

@app.get("/visibility_schedules", response_class=HTMLResponse, tags=["PAGE"])
def visibility_schedules(sat_id: int = Query(...)):
    """
    íŠ¹ì • ìœ„ì„±ì˜ ê´€ì¸¡ ê°€ëŠ¥ ì‹œê°„ëŒ€ë¥¼ HTMLë¡œ ë°˜í™˜í•˜ëŠ” í˜ì´ì§€
    """
    if sat_id not in satellites:
        return HTMLResponse(f"<p>Error: sat_id {sat_id} not found</p>", status_code=404)

    satellite = satellites[sat_id]
    results = []
    for name, gs in observer_locations.items():
        visible_periods = []
        visible = False
        start = None
        for offset in range(0, 7200, 30):
            future = sim_time + timedelta(seconds=offset)
            t = ts.utc(future.year, future.month, future.day, future.hour, future.minute, future.second)
            difference = satellite - gs
            topocentric = difference.at(t)
            alt, _, _ = topocentric.altaz()
            if alt.degrees >= threshold_deg:
                if not visible:
                    start = future
                    visible = True
            else:
                if visible:
                    visible_periods.append((start, future))
                    visible = False
        if visible and start:
            visible_periods.append((start, future))
        results.append((name, visible_periods))

    sections = []
    for name, periods in results:
        rows = ''.join(f"<tr><td>{start.strftime('%H:%M:%S')}</td><td>{end.strftime('%H:%M:%S')}</td></tr>" for start, end in periods)
        sections.append(f"<h2>{name}</h2><table><tr><th>Start</th><th>End</th></tr>{rows}</table>")

    return f"""
    <html>
    <head>
        <title>Visibility Schedule</title>
        <meta http-equiv="refresh" content="5">
        <style>
            body {{ font-family: Arial; margin: 2em; }}
            table {{ border-collapse: collapse; width: 60%; margin-bottom: 2em; }}
            th, td {{ border: 1px solid #ccc; padding: 8px; text-align: center; }}
        </style>
    </head>
    <body>
        <p><a href="/visibility_schedules/lists">â† Back to Satellite Visibility Schedule List</a></p>
        <h1>ğŸ“… Visibility Schedule for SAT{sat_id}</h1>
        <p><b>Sim Time:</b> {sim_time.isoformat()}</p>
        {''.join(sections)}
    </body>
    </html>
    """

@app.get("/iot_clusters", response_class=HTMLResponse, tags=["PAGE"])
def iot_clusters_ui():
    """
    IoT í´ëŸ¬ìŠ¤í„° ìœ„ì¹˜ë¥¼ HTMLë¡œ ë°˜í™˜í•˜ëŠ” í˜ì´ì§€
    """
    rows = []
    for name, loc in raw_iot_clusters.items():
        # console.log(f"Adding IoT cluster: {name} at {loc['latitude']}, {loc['longitude']}")
        rows.append(f"<tr><td>{name}</td><td>{loc['latitude']:.2f}</td><td>{loc['longitude']:.2f}</td></tr>")
    return f"""
    <html>
    <head>
        <title>IoT Clusters</title>
        <link rel=\"stylesheet\" href=\"https://unpkg.com/leaflet@1.9.4/dist/leaflet.css\" />
        <script src=\"https://unpkg.com/leaflet@1.9.4/dist/leaflet.js\"></script>
        <style>
            body {{ font-family: Arial; margin: 2em; }}
            table {{ border-collapse: collapse; width: 60%; }}
            th, td {{ border: 1px solid #ccc; padding: 8px; text-align: center; }}
            #map {{ height: 500px; margin-top: 30px; }}
        </style>
    </head>
    <body>
        <p><a href="/dashboard">â† Back to Dashboard</a></p>
        <h1>ğŸ“¡ IoT Cluster Locations</h1>
        <table>
            <tr><th>Name</th><th>Latitude</th><th>Longitude</th></tr>
            {''.join(rows)}
        </table>
        <hr/>
        <div id=\"map\"></div>
        <script>
            var map = L.map('map', {{
                center: [0, 0],
                zoom: 2,
                worldCopyJump: false,
                maxBounds: [[-85, -180], [85, 180]],
                maxBoundsViscosity: 1.0,
                inertia: false
            }});
            L.tileLayer('https://{{s}}.tile.openstreetmap.org/{{z}}/{{x}}/{{y}}.png', {{
                maxZoom: 18,
                attribution: 'Â© OpenStreetMap contributors'
            }}).addTo(map);

            const clusters = {raw_iot_clusters};
            console.log("IoT clusters:", clusters);
            for (let [name, loc] of Object.entries(clusters)) {{
                const lat = loc.latitude;
                const lon = loc.longitude;
                L.circleMarker([lat, lon],{{radius: 5, color: 'blue'}}).addTo(map);
                const marker = L.circleMarker([lat, lon], {{ radius: 5, color: 'blue' }}).addTo(map);
                marker.bindTooltip(name, {{
                    permanent: true,
                    direction: 'top',
                    className: 'iot-tooltip'
                }});
            }}
        </script>
    </body>
    </html>
    """

@app.get("/iot_visibility", response_class=HTMLResponse, tags=["PAGE"])
def iot_visibility():
    """
    IoT í´ëŸ¬ìŠ¤í„°ì—ì„œ ê´€ì¸¡ ê°€ëŠ¥í•œ ìœ„ì„± ëª©ë¡ì„ HTMLë¡œ ë°˜í™˜í•˜ëŠ” í˜ì´ì§€
    """
    iot_sections = []
    for name, iot in iot_clusters.items():
        t = ts.utc(sim_time.year, sim_time.month, sim_time.day, sim_time.hour, sim_time.minute, sim_time.second)
        rows = []
        for sid, sat in satellites.items():
            difference = sat - iot
            topocentric = difference.at(t)
            alt, az, dist = topocentric.altaz()
            if alt.degrees >= threshold_deg:
                rows.append(f'<tr><td>{sid}</td><td>{alt.degrees:.2f}Â°</td></tr>')
        table_html = f"""
        <h2>{name}</h2>
        <table>
            <tr><th>Sat ID</th><th>Elevation</th></tr>
            {''.join(rows)}
        </table>
        """
        iot_sections.append(table_html)

    return f"""
    <html>
    <head>
        <title>IOT Visibility</title>
        <meta http-equiv="refresh" content="5">
        <style>
            body {{ font-family: Arial; background: #f2f2f2; margin: 2em; }}
            h2 {{ margin-top: 2em; }}
            table {{ border-collapse: collapse; width: 60%; margin-bottom: 2em; }}
            th, td {{ border: 1px solid #ccc; padding: 8px; text-align: center; }}
        </style>
    </head>
    <body>
        <p><a href="/dashboard">â† Back to Dashboard</a></p>
        <h1>ğŸŒ IOT-wise Visible Satellites</h1>
        <p><b>Sim Time:</b> {sim_time.isoformat()}</p>
        <hr>
        {''.join(iot_sections)}
    </body>
    </html>
    """

@app.get("/comm_targets/lists", response_class=HTMLResponse, tags=["PAGE"])  
def comm_targets_list():
    """
    ìœ„ì„± ID ëª©ë¡ì„ ë³´ì—¬ì£¼ê³  ê° ìœ„ì„±ì˜ í†µì‹  ëŒ€ìƒì„ ìƒì„¸ í˜ì´ì§€ë¡œ ì—°ê²°í•˜ëŠ” ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€
    """
    links = [f'<li><a href="/comm_targets?sat_id={sid}">SAT{sid} Targets</a></li>' for sid in sorted(satellites)]
    return f"""
    <html>
    <head>
        <title>Comm Targets List</title>
        <meta http-equiv="refresh" content="5">
        <style>
            body {{ font-family: Arial; margin: 2em; }}
            ul {{ list-style-type: none; padding: 0; }}
            li {{ margin-bottom: 5px; }}
        </style>
    </head>
    <body>
        <p><a href="/dashboard">â† Back to Dashboard</a></p>
        <h1>ğŸš€ Satellite Comm Targets</h1>
        <ul>
            {''.join(links)}
        </ul>
    </body>
    </html>
    """

@app.get("/comm_targets", response_class=HTMLResponse, tags=["PAGE"])  
def comm_targets_detail(sat_id: int = Query(..., description="ìœ„ì„± ID")):
    """
    íŠ¹ì • ìœ„ì„±ì˜ í˜„ì¬ í†µì‹  ê°€ëŠ¥í•œ ì§€ìƒêµ­ê³¼ IoT í´ëŸ¬ìŠ¤í„°ë¥¼ HTML í…Œì´ë¸”ë¡œ ë³´ì—¬ì£¼ëŠ” ìƒì„¸ í˜ì´ì§€
    """
    if sat_id not in satellites:
        return HTMLResponse(f"<p style='color:red;'>Error: sat_id {sat_id} not found</p>", status_code=404)
    data = get_comm_targets(sat_id)
    # í…Œì´ë¸” ìƒì„±
    rows_ground = ''.join(f"<tr><td>{gs}</td></tr>" for gs in data['visible_ground_stations']) or '<tr><td>None</td></tr>'
    rows_iot    = ''.join(f"<tr><td>{ci}</td></tr>" for ci in data['visible_iot_clusters']) or '<tr><td>None</td></tr>'
    return f"""
    <html>
    <head>
        <title>Comm Targets for SAT{{sat_id}}</title>
        <meta http-equiv="refresh" content="5">
        <style>
            body {{ font-family: Arial; margin: 2em; }}
            table {{ border-collapse: collapse; width: 40%; margin-top: 1em; }}
            th, td {{ border: 1px solid #ccc; padding: 8px; text-align: left; }}
        </style>
    </head>
    <body>
        <p><a href="/comm_targets/lists">â† Back to Comm Targets List</a></p>
        <h1>ğŸ“¡ Comm Targets for SAT{sat_id}</h1>
        <p><b>Sim Time:</b> {data['sim_time']}</p>
        <h2>Visible Ground Stations</h2>
        <table><tr><th>Station</th></tr>{rows_ground}</table>
        <h2>Visible IoT Clusters</h2>
        <table><tr><th>Cluster</th></tr>{rows_iot}</table>
    </body>
    </html>
    """

# ==================== API ====================
@app.post("/api/reset_time", tags=["API"])
def reset_sim_time():
    """
    ì‹œë®¬ë ˆì´ì…˜ ì‹œê°„ì„ ì´ˆê¸°í™”í•˜ëŠ” API
    """
    global sim_time
    sim_time = datetime(2025, 3, 30, 0, 0, 0)
    return {"status": "reset", "sim_time": sim_time.isoformat()}


@app.get("/api/sim_time", tags=["API"])
def get_sim_time_api():
    """
    í˜„ì¬ ì‹œë®¬ë ˆì´ì…˜ ì‹œê°„ì„ ë°˜í™˜í•˜ëŠ” API
    """
    return {"sim_time": sim_time.isoformat()}


@app.put("/api/sim_time", tags=["API"])
def set_sim_time(year: int, month: int, day: int, hour: int = 0, minute: int = 0, second: int = 0):
    """
    ì‹œë®¬ë ˆì´ì…˜ ì‹œê°„ì„ ì„¤ì •í•˜ëŠ” API
    """
    global sim_time
    try:
        sim_time = datetime(year, month, day, hour, minute, second)
        return {"status": "updated", "sim_time": sim_time.isoformat()}
    except Exception as e:
        return {"error": str(e)}


@app.put("/api/set_step", tags=["API"])
def set_step(delta_sec: float = Query(...), interval_sec: float = Query(...)):
    """
    ì‹œë®¬ë ˆì´ì…˜ ë¸íƒ€ ì‹œê°„ ë° ë£¨í”„ ê°„ ì‹¤ì œ ëŒ€ê¸°ì‹œê°„ì„ ì„¤ì •
    """
    global sim_delta_sec, real_interval_sec
    if delta_sec <= 0 or interval_sec < 0:
        return {"error": "delta_secì€ ì–‘ìˆ˜, interval_secì€ 0 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤."}
    sim_delta_sec = float(delta_sec)
    real_interval_sec = float(interval_sec)
    return {"sim_delta_sec": sim_delta_sec, "real_interval_sec": real_interval_sec}


@app.post("/api/pause", tags=["API"])
def pause_simulation():
    """
    ì‹œë®¬ë ˆì´ì…˜ì„ ì¼ì‹œì •ì§€í•˜ëŠ” API
    """
    global sim_paused
    sim_paused = True
    return {"status": "paused"}


@app.post("/api/resume", tags=["API"])
def resume_simulation():
    """
    ì‹œë®¬ë ˆì´ì…˜ì„ ì¬ê°œí•˜ëŠ” API
    """
    global sim_paused
    sim_paused = False
    return {"status": "resumed"}


@app.get("/api/trajectory", tags=["API"])
def get_trajectory(sat_id: int = Query(...)):
    """
    íŠ¹ì • ìœ„ì„±ì˜ ê¶¤ì  ê²½ë¡œë¥¼ ë°˜í™˜í•˜ëŠ” API
    """
    if sat_id not in satellites:
        return {"error": f"sat_id {sat_id} not found"}
    satellite = satellites[sat_id]
    t0 = sim_time
    prev_lon = None
    segment: List[Dict[str, float]] = []
    segments: List[List[Dict[str, float]]] = []

    for offset_sec in range(0, 7200, 30):
        future = t0 + timedelta(seconds=offset_sec)
        t_ts = to_ts(future)
        subpoint = satellite.at(t_ts).subpoint()
        lat = subpoint.latitude.degrees
        lon = subpoint.longitude.degrees
        if prev_lon is not None and abs(lon - prev_lon) > 180:
            segments.append(segment)
            segment = []
        segment.append({"lat": lat, "lon": lon})
        prev_lon = lon

    if segment:
        segments.append(segment)
    return {"sat_id": sat_id, "segments": segments}


@app.get("/api/position", tags=["API"])
def get_position(sat_id: int = Query(...)):
    """
    íŠ¹ì • ìœ„ì„±ì˜ í˜„ì¬ ìœ„ì¹˜ë¥¼ ë°˜í™˜í•˜ëŠ” API
    """
    if sat_id not in current_sat_positions:
        return {"error": f"Position for SAT{sat_id} not available"}
    return current_sat_positions[sat_id]


@app.get("/api/comm_targets", tags=["API"])
def get_comm_targets(sat_id: int = Query(..., description="ìœ„ì„± ID")):
    """
    ì£¼ì–´ì§„ ìœ„ì„± IDì— ëŒ€í•´ í˜„ì¬ í†µì‹  ê°€ëŠ¥í•œ ì§€ìƒêµ­ê³¼ IoT í´ëŸ¬ìŠ¤í„°ë¥¼ ë°˜í™˜í•˜ëŠ” API
    """
    if sat_id not in satellites:
        return {"error": f"sat_id {sat_id} not found"}
    t_ts = get_current_time_utc()
    sat = satellites[sat_id]
    visible_ground = [name for name, gs in observer_locations.items()
                      if elevation_deg(sat, gs, t_ts) >= threshold_deg]
    visible_iot = [name for name, cluster in iot_clusters.items()
                   if elevation_deg(sat, cluster, t_ts) >= threshold_deg]
    return {
        "sim_time": sim_time.isoformat(),
        "sat_id": sat_id,
        "visible_ground_stations": visible_ground,
        "visible_iot_clusters": visible_iot,
    }


@app.get("/api/gs/visibility", tags=["API/GS"])
def get_gs_visibility():
    """
    í˜„ì¬ ì‹œë®¬ë ˆì´ì…˜ ì‹œê°„ì— ê° ì§€ìƒêµ­ì—ì„œ ê´€ì¸¡ ê°€ëŠ¥í•œ ìœ„ì„± ëª©ë¡ì„ ë°˜í™˜í•˜ëŠ” API
    """
    result = {}
    t_ts = get_current_time_utc()
    for name, gs in observer_locations.items():
        visible_sats = []
        for sid, sat in satellites.items():
            alt_deg = elevation_deg(sat, gs, t_ts)
            if alt_deg >= threshold_deg:
                visible_sats.append({"sat_id": sid, "elevation": alt_deg})
        result[name] = visible_sats
    return {"sim_time": sim_time.isoformat(), "data": result}


@app.get("/api/gs/visibility_schedule", tags=["API/GS"])
def get_visibility_schedule(sat_id: int = Query(...)):
    """
    íŠ¹ì • ìœ„ì„±ì˜ ê´€ì¸¡ ê°€ëŠ¥ ì‹œê°„ëŒ€ë¥¼ ë°˜í™˜í•˜ëŠ” API
    """
    if sat_id not in satellites:
        return {"error": f"sat_id {sat_id} not found"}
    satellite = satellites[sat_id]
    results = {}

    for name, gs in observer_locations.items():
        visible_periods: List[tuple[str, str]] = []
        visible = False
        start: Optional[datetime] = None
        for offset in range(0, 7200, 30):
            future = sim_time + timedelta(seconds=offset)
            t_ts = to_ts(future)
            alt_deg = elevation_deg(satellite, gs, t_ts)
            if alt_deg >= threshold_deg:
                if not visible:
                    start = future
                    visible = True
            else:
                if visible:
                    visible_periods.append((start.isoformat(), future.isoformat()))
                    visible = False
        if visible and start:
            visible_periods.append((start.isoformat(), future.isoformat()))
        results[name] = visible_periods

    return {"sim_time": sim_time.isoformat(), "sat_id": sat_id, "schedule": results}


@app.get("/api/gs/next_comm", tags=["API/GS"])
def get_next_comm_for_sat(sat_id: int = Query(..., description="ìœ„ì„± ID")):
    """
    ì£¼ì–´ì§„ sat_id ìœ„ì„±ì´ ë‹¤ìŒì— í†µì‹  ê°€ëŠ¥í•œ ì§€ìƒêµ­ê³¼ ì‹œê°„(ì‹œë®¬ë ˆì´ì…˜ ì‹œê°„)ì„ ë°˜í™˜í•˜ëŠ” API
    """
    if sat_id not in satellites:
        return {"error": f"sat_id {sat_id} not found"}
    sat = satellites[sat_id]
    t0 = sim_time
    horizon = 86400
    step = max(1, int(sim_delta_sec))

    for offset in range(step, horizon + step, step):
        t_future = t0 + timedelta(seconds=offset)
        if any(elevation_deg(sat, gs, to_ts(t_future)) >= threshold_deg for gs in observer_locations.values()):
            # ê°€ì¥ ë¨¼ì € í†µì‹ ë˜ëŠ” ì§€ìƒêµ­ ì´ë¦„ë„ í•¨ê»˜ ì°¾ê¸°
            for name, gs in observer_locations.items():
                if elevation_deg(sat, gs, to_ts(t_future)) >= threshold_deg:
                    return {"sat_id": sat_id, "ground_station": name, "next_comm_time": t_future.isoformat()}
    return {"sat_id": sat_id, "next_comm_time": None, "message": "ë‹¤ìŒ 24ì‹œê°„ ë‚´ í†µì‹  ì°½ ì—†ìŒ"}


@app.get("/api/gs/next_comm_all", tags=["API/GS"])
def get_next_comm_all():
    """
    ëª¨ë“  ìœ„ì„±ì— ëŒ€í•´ ë‹¤ìŒ í†µì‹  ê°€ëŠ¥ ì‹œê°„(ì§€ìƒêµ­, ì‹œë®¬ë ˆì´ì…˜ ì‹œê°„)ì„ ë°˜í™˜í•˜ëŠ” API
    """
    t0 = sim_time
    horizon = 86400
    step = max(1, int(sim_delta_sec))
    result = {}

    for sat_id, sat in satellites.items():
        next_time = None
        next_gs = None
        for offset in range(step, horizon + step, step):
            t_future = t0 + timedelta(seconds=offset)
            for name, gs in observer_locations.items():
                if elevation_deg(sat, gs, to_ts(t_future)) >= threshold_deg:
                    next_time = t_future
                    next_gs = name
                    break
            if next_time:
                break
        result[sat_id] = {
            "ground_station": next_gs,
            "next_comm_time": next_time.isoformat() if next_time else None
        }
    return {"sim_time": sim_time.isoformat(), "next_comm": result}


@app.put("/api/observer", tags=["API/Observer"])
def set_observer(name: str = Query(...)):
    """
    ì§€ìƒêµ­ ê´€ì¸¡ ìœ„ì¹˜ë¥¼ ì„¤ì •í•˜ëŠ” API
    """
    global observer, current_observer_name
    if name not in observer_locations:
        return {"error": f"observer '{name}' is not supported"}
    observer = observer_locations[name]
    current_observer_name = name
    return {"status": "ok", "observer": name}


@app.get("/api/observer/check_comm", tags=["API/Observer"])
def get_observer_check_comm(sat_id: int = Query(...)):
    """
    ìœ„ì„± í†µì‹  ê°€ëŠ¥ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ëŠ” API
    """
    available = bool(sat_comm_status.get(sat_id, False))
    return {"sat_id": sat_id, "observer": current_observer_name, "sim_time": sim_time.isoformat(), "available": available}


@app.get("/api/observer/all_visible", tags=["API/Observer"])
def get_all_visible():
    """
    í˜„ì¬ ì‹œë®¬ë ˆì´ì…˜ ì‹œê°„ì— ì§€ìƒêµ­(observer)ì—ì„œ ê´€ì¸¡ ê°€ëŠ¥í•œ ëª¨ë“  ìœ„ì„±ì˜ IDë¥¼ ë°˜í™˜í•˜ëŠ” API
    """
    visible_sats = [sat_id for sat_id, available in sat_comm_status.items() if bool(available)]
    return {"observer": current_observer_name, "sim_time": sim_time.isoformat(), "visible_sat_ids": visible_sats}


@app.get("/api/observer/visible_count", tags=["API/Observer"])
def get_visible_count():
    """
    í˜„ì¬ ì‹œë®¬ë ˆì´ì…˜ ì‹œê°„ì— ì§€ìƒêµ­(observer)ì—ì„œ ê´€ì¸¡ ê°€ëŠ¥í•œ ìœ„ì„±ì˜ ê°œìˆ˜ë¥¼ ë°˜í™˜í•˜ëŠ” API
    """
    count = sum(1 for available in sat_comm_status.values() if bool(available))
    return {"observer": current_observer_name, "sim_time": sim_time.isoformat(), "visible_count": count}


@app.get("/api/observer/elevation", tags=["API/Observer"])
def get_elevation(sat_id: int = Query(...)):
    """
    íŠ¹ì • ìœ„ì„±ì˜ í˜„ì¬ ì‹œë®¬ë ˆì´ì…˜ ì‹œê°„ì— ëŒ€í•œ ì§€ìƒêµ­ê³¼ì˜ ê³ ë„ë¥¼ ë°˜í™˜í•˜ëŠ” API
    """
    satellite = satellites.get(sat_id)
    if satellite is None:
        return {"error": f"sat_id {sat_id} not found"}
    alt_deg = elevation_deg(satellite, observer, get_current_time_utc())
    return {"sat_id": sat_id, "observer": current_observer_name, "sim_time": sim_time.isoformat(), "elevation_deg": alt_deg}


@app.get("/api/observer/next_comm", tags=["API/Observer"])
def get_next_comm_with_observer(sat_id: int = Query(..., description="ìœ„ì„± ID")):
    """
    ì£¼ì–´ì§„ sat_id ìœ„ì„±ì´ í˜„ì¬ observerì™€ ë‹¤ìŒì— í†µì‹  ê°€ëŠ¥í•œ ì‹œê°„ì„ ë°˜í™˜í•˜ëŠ” API
    """
    if sat_id not in satellites:
        return {"error": f"sat_id {sat_id} not found"}
    sat = satellites[sat_id]
    t0 = sim_time
    horizon = 86400
    step = max(1, int(sim_delta_sec))

    for offset in range(step, horizon + step, step):
        t_future = t0 + timedelta(seconds=offset)
        if elevation_deg(sat, observer, to_ts(t_future)) >= threshold_deg:
            return {"sat_id": sat_id, "observer": current_observer_name, "next_comm_time": t_future.isoformat()}
    return {"sat_id": sat_id, "observer": current_observer_name, "next_comm_time": None, "message": "ë‹¤ìŒ 24ì‹œê°„ ë‚´ í†µì‹  ì°½ ì—†ìŒ"}


@app.get("/api/observer/next_comm_all", tags=["API/Observer"])
def get_next_comm_all_with_observer():
    """
    ëª¨ë“  ìœ„ì„±ì— ëŒ€í•´ í˜„ì¬ observer ì™€ì˜ ë‹¤ìŒ í†µì‹  ê°€ëŠ¥í•œ ì‹œê°„ì„ ë°˜í™˜í•˜ëŠ” API
    """
    t0 = sim_time
    horizon = 86400
    step = max(1, int(sim_delta_sec))
    result = {}

    for sat_id, sat in satellites.items():
        next_time = None
        for offset in range(step, horizon + step, step):
            t_future = t0 + timedelta(seconds=offset)
            if elevation_deg(sat, observer, to_ts(t_future)) >= threshold_deg:
                next_time = t_future.isoformat()
                break
        result[sat_id] = {"next_comm_time": next_time}

    return {"observer": current_observer_name, "sim_time": sim_time.isoformat(), "next_comm": result}


@app.get("/api/iot_clusters/position", tags=["API/IoT"])
def get_iot_clusters_position():
    """
    IoT í´ëŸ¬ìŠ¤í„°ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ ë°˜í™˜í•˜ëŠ” API
    """
    # ToposëŠ” ì§ë ¬í™” ë¶ˆê°€ â†’ lat/lonë§Œ ë°˜í™˜
    clusters = {name: {"lat": t.latitude.degrees, "lon": t.longitude.degrees, "elev_m": raw_iot_clusters[name]["elevation_m"]}
                for name, t in iot_clusters.items()}
    return {"sim_time": sim_time.isoformat(), "clusters": clusters}


@app.get("/api/iot_clusters/visibility_schedule", tags=["API/IoT"])
def get_iot_clusters_visibility(sat_id: int = Query(...)):
    """
    íŠ¹ì • ìœ„ì„±ì´ IoT í´ëŸ¬ìŠ¤í„°ì—ì„œ ê´€ì¸¡ ê°€ëŠ¥í•œì§€ ì—¬ë¶€ì™€ ê´€ì¸¡ ê°€ëŠ¥ ì‹œê°„ëŒ€ë¥¼ ë°˜í™˜í•˜ëŠ” API
    """
    if sat_id not in satellites:
        return {"error": f"sat_id {sat_id} not found"}
    satellite = satellites[sat_id]
    result = []

    for name, cluster in iot_clusters.items():
        visible_periods: List[tuple[str, str]] = []
        visible = False
        start: Optional[datetime] = None
        for offset in range(0, 7200, 30):
            future = sim_time + timedelta(seconds=offset)
            if elevation_deg(satellite, cluster, to_ts(future)) >= threshold_deg:
                if not visible:
                    start = future
                    visible = True
            else:
                if visible:
                    visible_periods.append((start.isoformat(), future.isoformat()))
                    visible = False
        if visible and start:
            visible_periods.append((start.isoformat(), future.isoformat()))
        if visible_periods:
            result.append({"iot_cluster": name, "periods": visible_periods})
    return {"sim_time": sim_time.isoformat(), "sat_id": sat_id, "schedule": result}


@app.get("/api/iot_clusters/visible", tags=["API/IoT"])
def get_iot_clusters_visible(
    sat_id: Optional[int] = Query(None, description="ìœ„ì„± ID"),
    iot_name: Optional[str] = Query(None, description="IoT í´ëŸ¬ìŠ¤í„° ì´ë¦„")
):
    """
    íŠ¹ì • IoT í´ëŸ¬ìŠ¤í„°ì—ì„œ ê´€ì¸¡ ê°€ëŠ¥í•œì§€ ì—¬ë¶€ë¥¼ ë°˜í™˜í•˜ëŠ” API\n
    - sat_idë§Œ ì§€ì • â†’ í•´ë‹¹ ìœ„ì„±ê³¼ í†µì‹  ê°€ëŠ¥í•œ IoT í´ëŸ¬ìŠ¤í„° ëª©ë¡ ë°˜í™˜
    - iot_nameë§Œ ì§€ì • â†’ í•´ë‹¹ IoT í´ëŸ¬ìŠ¤í„°ì™€ í†µì‹  ê°€ëŠ¥í•œ ìœ„ì„± ID ëª©ë¡ ë°˜í™˜
    """
    if (sat_id is None) == (iot_name is None):
        return {"error": "neither sat_id or iot_name not found"}

    t_ts = get_current_time_utc()

    if sat_id is not None:
        if sat_id not in satellites:
            return {"error": f"sat_id {sat_id} not found"}
        satellite = satellites[sat_id]
        visible_clusters = [name for name, cluster in iot_clusters.items()
                            if elevation_deg(satellite, cluster, t_ts) >= threshold_deg]
        return {"sat_id": sat_id, "sim_time": sim_time.isoformat(), "visible_iot_clusters": visible_clusters}

    cluster_name = iot_name
    if cluster_name not in iot_clusters:
        return {"error": f"iot cluster '{cluster_name}' not found"}
    cluster = iot_clusters[cluster_name]
    visible_sats = [sid for sid, sat in satellites.items()
                    if elevation_deg(sat, cluster, t_ts) >= threshold_deg]
    return {"sim_time": sim_time.isoformat(), "iot_cluster": cluster_name, "visible_sat_ids": visible_sats}


@app.get("/api/iot_clusters/visible_count", tags=["API/IoT"])
def get_iot_clusters_visible_count(iot_name: str = Query(...)):
    """
    íŠ¹ì • IoT í´ëŸ¬ìŠ¤í„°ì—ì„œ ê´€ì¸¡ ê°€ëŠ¥í•œ ìœ„ì„±ì˜ ê°œìˆ˜ë¥¼ ë°˜í™˜í•˜ëŠ” API
    """
    if iot_name not in iot_clusters:
        return {"error": f"iot cluster '{iot_name}' not found"}
    cluster = iot_clusters[iot_name]
    t_ts = get_current_time_utc()
    count = sum(1 for _, sat in satellites.items() if elevation_deg(sat, cluster, t_ts) >= threshold_deg)
    return {"sim_time": sim_time.isoformat(), "iot_cluster": iot_name, "visible_count": count}

@app.get("/api/data/summary", tags=["API/Data"])
def get_data_summary(
    detail: bool = Query(False, description="ìœ„ì„±(í´ë¼ì´ì–¸íŠ¸)ë³„ ìƒì„¸ ëª©ë¡ í¬í•¨"),
    histogram: bool = Query(False, description="ìœ„ì„±ë³„ í´ë˜ìŠ¤ ë¶„í¬ í¬í•¨(10 í´ë˜ìŠ¤)"),
    limit: int = Query(20, ge=1, le=1000, description="detail=falseì¼ ë•Œ ë‚˜ì—´í•  ìœ„ì„± ìˆ˜"),
):
    """
    CIFAR-10 ë¶„í• /í• ë‹¹ ìƒíƒœ ìš”ì•½
    - ê¸°ë³¸: ì „ì—­ ì„¤ì •/ìˆ˜ëŸ‰ ìš”ì•½ë§Œ ë°˜í™˜
    - detail=true: ìœ„ì„±ë³„ ìƒ˜í”Œ ìˆ˜ í¬í•¨(ê¸°ë³¸ ìµœëŒ€ limitê°œ)
    - histogram=true: ìœ„ì„±ë³„ í´ë˜ìŠ¤ íˆìŠ¤í† ê·¸ë¨(0~9) í¬í•¨
    """
    # ë°ì´í„° ë ˆì§€ìŠ¤íŠ¸ë¦¬/í—¬í¼ í™•ì¸
    if "DATA_REGISTRY" not in globals() or "get_training_dataset" not in globals():
        return {"error": "DATA_REGISTRY / get_training_dataset ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

    # êµ¬ì„±ê°’/í™˜ê²½
    try:
        cfg = {
            "samples_per_client": SAMPLES_PER_CLIENT,
            "dirichlet_alpha": DIRICHLET_ALPHA,
            "seed": RNG_SEED,
            "with_replacement": WITH_REPLACEMENT,
            "cifar_root": str(CIFAR_ROOT),
        }
    except Exception as e:
        cfg = {"error": f"config fetch failed: {e}"}

    # ìœ„ì„±/í• ë‹¹ í˜„í™©
    sat_ids = sorted(satellites.keys())
    assignments = getattr(DATA_REGISTRY, "assignments", {})
    num_clients_assigned = len(assignments) if isinstance(assignments, dict) else len(sat_ids)

    result: Dict[str, Any] = {
        "dataset": "CIFAR-10",
        "config": cfg,
        "counts": {
            "num_satellites": len(sat_ids),
            "num_clients_assigned": num_clients_assigned,
        },
        "per_client": [],
        "notes": [
            "detail=true ë¡œ ì „ì²´ ìœ„ì„± ë³„ ìƒ˜í”Œ ìˆ˜ë¥¼ í¬í•¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "histogram=true ë¡œ (0~9) í´ë˜ìŠ¤ ë¶„í¬ë¥¼ í¬í•¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "detail=false ì¸ ê²½ìš° limit ë§Œí¼ë§Œ ë‚˜ì—´í•©ë‹ˆë‹¤.",
        ],
    }

    # ìƒì„¸ê°€ ì•„ë‹ˆë©´ ìƒ˜í”Œ ìˆ˜ ì§‘ê³„ë¥¼ ê±´ë„ˆë›°ê³  ë
    if not detail and not histogram:
        return result

    # ìƒì„¸ ëª©ë¡ ë§Œë“¤ê¸° (detail=falseë©´ limit ë§Œí¼ë§Œ)
    target_ids = sat_ids if detail else sat_ids[:limit]
    total_listed_samples = 0

    for sid in target_ids:
        try:
            ds = get_training_dataset(sid)  # TensorDataset ì˜ˆìƒ
            n = len(ds) if hasattr(ds, "__len__") else None
            entry: Dict[str, Any] = {"sat_id": sid, "num_samples": n}
            total_listed_samples += (n or 0)

            if histogram and n and hasattr(ds, "tensors") and len(ds.tensors) >= 2:
                labels = ds.tensors[1]
                hist: Dict[int, int] = {}
                try:
                    # torchê°€ ìˆë‹¤ë©´ torchë¡œ
                    if _has_torch:
                        import torch
                        uniq, cnt = torch.unique(labels.cpu(), return_counts=True)
                        hist = {int(u.item()): int(c.item()) for u, c in zip(uniq, cnt)}
                    else:
                        # torch ë¯¸ì‚¬ìš© ëŒ€ë¹„
                        arr = labels.detach().cpu().numpy()
                        c = Counter(arr.tolist())
                        hist = {int(k): int(v) for k, v in c.items()}
                except Exception as e:
                    entry["hist_error"] = f"{e}"
                else:
                    # 0~9 í´ë˜ìŠ¤ ëˆ„ë½ëœ í•­ëª© 0ìœ¼ë¡œ ì±„ìš°ê¸°(ë³´ê¸° ì¢‹ê²Œ)
                    for cls in range(10):
                        hist.setdefault(cls, 0)
                    entry["class_hist"] = hist
            result["per_client"].append(entry)
        except Exception as e:
            result["per_client"].append({"sat_id": sid, "error": str(e)})

    # ì°¸ê³  í•©ê³„(ë‚˜ì—´ëœ í•­ëª© ê¸°ì¤€)
    result["counts"]["total_samples_listed"] = total_listed_samples
    if not detail:
        result["counts"]["listed_clients"] = len(target_ids)

    return result

@app.get("/api/model/global", tags=["API/Model"])
def get_global_model_info(detail: bool = Query(False, description="ëª¨ë“  ê¸€ë¡œë²Œ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡/ë©”íƒ€ë°ì´í„° í¬í•¨")):
    """
    ê¸€ë¡œë²Œ ëª¨ë¸ì˜ í˜„ì¬ ë²„ì „/ê²½ë¡œë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    - version: GLOBAL_MODEL_VERSION (ì´ˆê¸°í™” ì „ì´ë©´ -1)
    - latest_ckpt: CKPT_DIR/global_v{version}.ckpt ê°€ ì¡´ì¬í•˜ë©´ ê·¸ ê²½ë¡œë¥¼, ì—†ìœ¼ë©´ ê°€ì¥ ìµœê·¼ íŒŒì¼ì„ ì°¾ì•„ ë°˜í™˜
    - detail=true ì¼ ë•Œ: CKPT_DIR ì•ˆì˜ global_v*.ckpt íŒŒì¼ ë¦¬ìŠ¤íŠ¸ì™€ íŒŒì¼ ë©”íƒ€(í¬ê¸°, mtime)ë¥¼ í•¨ê»˜ ë°˜í™˜
    """
    # ì•ˆì „ì¥ì¹˜: í•„ìš”í•œ ì „ì—­ì´ ì—†ì„ ìˆ˜ ìˆì„ ë•Œ ê¸°ë³¸ê°’
    version = globals().get("GLOBAL_MODEL_VERSION", -1)
    ckpt_dir = globals().get("CKPT_DIR", Path(__file__).parent / "ckpt")
    ckpt_dir.mkdir(parents=True, exist_ok=True)

    # ìš°ì„  canonical ê²½ë¡œ(global_v{ver}.ckpt)ë¥¼ ì‹œë„
    canonical = ckpt_dir / f"global_v{version}.ckpt" if version >= 0 else None
    latest_path = canonical if (canonical and canonical.exists()) else None

    # ì—†ìœ¼ë©´ ê°€ì¥ ìµœì‹  íŒŒì¼ì„ ìŠ¤ìº”
    if latest_path is None:
        # _find_latest_global_ckpt()ê°€ ìˆë‹¤ë©´ í™œìš©
        if "_find_latest_global_ckpt" in globals():
            try:
                best_ver, best_path = _find_latest_global_ckpt()
                if best_ver is not None and best_path is not None and best_path.exists():
                    version = max(version, best_ver)
                    latest_path = best_path
            except Exception:
                pass
        # ê·¸ë˜ë„ ëª» ì°¾ìœ¼ë©´ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ìŠ¤ìº”
        if latest_path is None:
            candidates = sorted(ckpt_dir.glob("global_v*.ckpt"))
            if candidates:
                latest_path = candidates[-1]

    resp = {
        "version": int(version),
        "latest_ckpt": str(latest_path) if latest_path else None,
        "initialized": bool(version >= 0 and latest_path is not None),
    }

    if detail:
        files = []
        for p in sorted(ckpt_dir.glob("global_v*.ckpt")):
            try:
                stat = p.stat()
                files.append({
                    "path": str(p),
                    "size_bytes": stat.st_size,
                    "mtime_epoch": int(stat.st_mtime),
                    "mtime_iso": time.strftime("%Y-%m-%dT%H:%M:%S", time.localtime(stat.st_mtime)),
                })
            except Exception:
                files.append({"path": str(p)})
        resp["files"] = files

    return resp


@app.get("/api/model/global/version", tags=["API/Model"])
def get_global_model_version():
    """
    ê¸€ë¡œë²Œ ëª¨ë¸ ë²„ì „ ìˆ«ìë§Œ ê°„ë‹¨íˆ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    version = globals().get("GLOBAL_MODEL_VERSION", -1)
    return {"version": int(version)}

# ==================== ê¸°íƒ€ ====================
async def auto_resume_after_delay():
    global sim_paused, auto_resume_delay_sec
    await asyncio.sleep(auto_resume_delay_sec)
    sim_paused = False
    auto_resume_delay_sec = 0
